{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":12816487,"sourceType":"datasetVersion","datasetId":8104328}],"dockerImageVersionId":31089,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":5,"nbformat":4,"cells":[{"id":"f58087d0","cell_type":"code","source":"# Import các thư viện cần thiết\n\n# Standard library imports\nimport os\nimport csv\nimport copy\nimport math\nimport random\nimport shutil\nimport time\nfrom os import environ\nfrom platform import system\n\n# Third-party imports\nimport cv2\nimport numpy as np\nimport yaml\nfrom tqdm import tqdm\nfrom PIL import Image\nimport matplotlib.pyplot as plt\n\n# PyTorch imports\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nimport torchvision\nfrom torch.utils.data import Dataset as TorchDataset, DataLoader, Subset\nfrom torchvision.ops import box_iou, nms\n\n# Optional imports\ntry:\n    import onnx\nexcept ImportError:\n    onnx = None\n\ntry:\n    import albumentations\nexcept ImportError:\n    albumentations = None\n\ntry:\n    from roboflow import Roboflow\nexcept ImportError:\n    os.system(\"pip install roboflow\")\n    from roboflow import Roboflow\n\n# Device configuration\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint(f\"Using device: {device}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-23T07:55:11.218993Z","iopub.execute_input":"2025-08-23T07:55:11.219632Z","iopub.status.idle":"2025-08-23T07:55:11.227168Z","shell.execute_reply.started":"2025-08-23T07:55:11.219599Z","shell.execute_reply":"2025-08-23T07:55:11.226268Z"}},"outputs":[{"name":"stdout","text":"Using device: cuda\n","output_type":"stream"}],"execution_count":41},{"id":"19951b5a","cell_type":"code","source":"try:\n    DRIVE_SAVE_PATH = \"/kaggle/working/\"\n    os.makedirs(DRIVE_SAVE_PATH, exist_ok=True)\n\n    SAVE_PATH = os.path.join(DRIVE_SAVE_PATH, \"custom_yolo_model.pth\")\n    DATASET_PATH = \"/kaggle/input/wild-animals-detection-yolov8\"  # Fixed path\n\n    CHECKPOINT_DIR = os.path.join(DRIVE_SAVE_PATH, \"checkpoints\")\n    os.makedirs(CHECKPOINT_DIR, exist_ok=True)\n\nexcept:\n    SAVE_PATH = \"./custom_yolo_model.pth\"\n    DATASET_PATH = \"./roboflow_dataset\"\n    CHECKPOINT_DIR = \"./checkpoints\"\n    os.makedirs(CHECKPOINT_DIR, exist_ok=True)\n\nRESUME_TRAINING = True\nSAVE_CHECKPOINT_EVERY = 10","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-23T07:55:11.228516Z","iopub.execute_input":"2025-08-23T07:55:11.228716Z","iopub.status.idle":"2025-08-23T07:55:11.246520Z","shell.execute_reply.started":"2025-08-23T07:55:11.228702Z","shell.execute_reply":"2025-08-23T07:55:11.245899Z"}},"outputs":[],"execution_count":42},{"id":"c226266f","cell_type":"markdown","source":"# KIẾN TRÚC","metadata":{}},{"id":"401e7601","cell_type":"code","source":"class Conv(nn.Module):\n    def __init__(self,in_channels, out_channels,kernel_size=3,stride=1,padding=1,groups=1,activation=True):\n        super().__init__()\n        self.conv=nn.Conv2d(in_channels,out_channels,kernel_size,stride,padding,bias=False,groups=groups)\n        self.bn=nn.BatchNorm2d(out_channels,eps=0.001,momentum=0.03)\n        self.act=nn.SiLU(inplace=True) if activation else nn.Identity()\n\n    def forward(self,x):\n        return self.act(self.bn(self.conv(x)))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-23T07:55:11.247274Z","iopub.execute_input":"2025-08-23T07:55:11.247534Z","iopub.status.idle":"2025-08-23T07:55:11.260425Z","shell.execute_reply.started":"2025-08-23T07:55:11.247513Z","shell.execute_reply":"2025-08-23T07:55:11.259862Z"}},"outputs":[],"execution_count":43},{"id":"8a2cd26d","cell_type":"code","source":"# 2.1 Bottleneck: staack of 2 COnv with shortcut connnection (True/False)\nclass Bottleneck(nn.Module):\n    def __init__(self,in_channels,out_channels,shortcut=True):\n        super().__init__()\n        self.conv1=Conv(in_channels,out_channels,kernel_size=3,stride=1,padding=1)\n        self.conv2=Conv(out_channels,out_channels,kernel_size=3,stride=1,padding=1)\n        self.shortcut=shortcut\n\n    def forward(self,x):\n        x_in=x # for residual connection\n        x=self.conv1(x)\n        x=self.conv2(x)\n        if self.shortcut:\n            x=x+x_in\n        return x\n    \n# 2.2 C2f: Conv + bottleneck*N+ Conv\nclass C2f(nn.Module):\n    def __init__(self,in_channels,out_channels, num_bottlenecks,shortcut=True):\n        super().__init__()\n        \n        self.mid_channels=out_channels//2\n        self.num_bottlenecks=num_bottlenecks\n\n        self.conv1=Conv(in_channels,out_channels,kernel_size=1,stride=1,padding=0)\n        \n        # sequence of bottleneck layers\n        self.m=nn.ModuleList([Bottleneck(self.mid_channels,self.mid_channels) for _ in range(num_bottlenecks)])\n\n        self.conv2=Conv((num_bottlenecks+2)*out_channels//2,out_channels,kernel_size=1,stride=1,padding=0)\n    \n    def forward(self,x):\n        x=self.conv1(x)\n\n        # split x along channel dimension\n        x1,x2=x[:,:x.shape[1]//2,:,:], x[:,x.shape[1]//2:,:,:]\n        \n        # list of outputs\n        outputs=[x1,x2] # x1 is fed through the bottlenecks\n\n        for i in range(self.num_bottlenecks):\n            x1=self.m[i](x1)    # [bs,0.5c_out,w,h]\n            outputs.insert(0,x1)\n\n        outputs=torch.cat(outputs,dim=1) # [bs,0.5c_out(num_bottlenecks+2),w,h]\n        out=self.conv2(outputs)\n\n        return out","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-23T07:55:11.261822Z","iopub.execute_input":"2025-08-23T07:55:11.262168Z","iopub.status.idle":"2025-08-23T07:55:11.277971Z","shell.execute_reply.started":"2025-08-23T07:55:11.262151Z","shell.execute_reply":"2025-08-23T07:55:11.277275Z"}},"outputs":[],"execution_count":44},{"id":"f9b8e182","cell_type":"code","source":"class SPPF(nn.Module):\n    def __init__(self,in_channels,out_channels,kernel_size=5):\n        #kernel_size= size of maxpool\n        super().__init__()\n        hidden_channels=in_channels//2\n        self.conv1=Conv(in_channels,hidden_channels,kernel_size=1,stride=1,padding=0)\n        # concatenate outputs of maxpool and feed to conv2\n        self.conv2=Conv(4*hidden_channels,out_channels,kernel_size=1,stride=1,padding=0)\n\n        # maxpool is applied at 3 different sacles\n        self.m=nn.MaxPool2d(kernel_size=kernel_size,stride=1,padding=kernel_size//2,dilation=1,ceil_mode=False)\n    \n    def forward(self,x):\n        x=self.conv1(x)\n\n        # apply maxpooling at diffent scales\n        y1=self.m(x)\n        y2=self.m(y1)\n        y3=self.m(y2)\n\n        # concantenate \n        y=torch.cat([x,y1,y2,y3],dim=1)\n\n        # final conv\n        y=self.conv2(y)\n\n        return y\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-23T07:55:11.278763Z","iopub.execute_input":"2025-08-23T07:55:11.278950Z","iopub.status.idle":"2025-08-23T07:55:11.297534Z","shell.execute_reply.started":"2025-08-23T07:55:11.278935Z","shell.execute_reply":"2025-08-23T07:55:11.296855Z"}},"outputs":[],"execution_count":45},{"id":"0401290f","cell_type":"code","source":"# backbone = DarkNet53\n\n# return d,w,r based on version\ndef yolo_params(version):\n    if version=='n':\n        return 1/3,1/4,2.0\n    elif version=='s':\n        return 1/3,1/2,2.0\n    elif version=='m':\n        return 2/3,3/4,1.5\n    elif version=='l':\n        return 1.0,1.0,1.0\n    elif version=='x':\n        return 1.0,1.25,1.0\n    \nclass Backbone(nn.Module):\n    def __init__(self,version,in_channels=3,shortcut=True):\n        super().__init__()\n        d,w,r=yolo_params(version)\n\n        # conv layers\n        self.conv_0=Conv(in_channels,int(64*w),kernel_size=3,stride=2,padding=1)\n        self.conv_1=Conv(int(64*w),int(128*w),kernel_size=3,stride=2,padding=1)\n        self.conv_3=Conv(int(128*w),int(256*w),kernel_size=3,stride=2,padding=1)\n        self.conv_5=Conv(int(256*w),int(512*w),kernel_size=3,stride=2,padding=1)\n        self.conv_7=Conv(int(512*w),int(512*w*r),kernel_size=3,stride=2,padding=1)\n\n        # c2f layers\n        self.c2f_2=C2f(int(128*w),int(128*w),num_bottlenecks=int(3*d),shortcut=True)\n        self.c2f_4=C2f(int(256*w),int(256*w),num_bottlenecks=int(6*d),shortcut=True)\n        self.c2f_6=C2f(int(512*w),int(512*w),num_bottlenecks=int(6*d),shortcut=True)\n        self.c2f_8=C2f(int(512*w*r),int(512*w*r),num_bottlenecks=int(3*d),shortcut=True)\n\n        # sppf\n        self.sppf=SPPF(int(512*w*r),int(512*w*r))\n    \n    def forward(self,x):\n        x=self.conv_0(x)\n        x=self.conv_1(x)\n\n        x=self.c2f_2(x)\n\n        x=self.conv_3(x)\n\n        out1=self.c2f_4(x) # keep for output\n\n        x=self.conv_5(out1)\n\n        out2=self.c2f_6(x) # keep for output\n\n        x=self.conv_7(out2)\n        x=self.c2f_8(x)\n        out3=self.sppf(x)\n\n        return out1,out2,out3\n\nprint(\"----Nano model -----\")\nbackbone_n=Backbone(version='n')\nprint(f\"{sum(p.numel() for p in backbone_n.parameters())/1e6} million parameters\")\n\nprint(\"----Small model -----\")\nbackbone_s=Backbone(version='s')\nprint(f\"{sum(p.numel() for p in backbone_s.parameters())/1e6} million parameters\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-23T07:55:11.337820Z","iopub.execute_input":"2025-08-23T07:55:11.338263Z","iopub.status.idle":"2025-08-23T07:55:11.421483Z","shell.execute_reply.started":"2025-08-23T07:55:11.338245Z","shell.execute_reply":"2025-08-23T07:55:11.420894Z"}},"outputs":[{"name":"stdout","text":"----Nano model -----\n1.272656 million parameters\n----Small model -----\n5.079712 million parameters\n","output_type":"stream"}],"execution_count":46},{"id":"dee2927c","cell_type":"code","source":"# upsample = nearest-neighbor interpolation with scale_factor=2\n#            doesn't have trainable paramaters\nclass Upsample(nn.Module):\n    def __init__(self,scale_factor=2,mode='nearest'):\n        super().__init__()\n        self.scale_factor=scale_factor\n        self.mode=mode\n\n    def forward(self,x):\n        return nn.functional.interpolate(x,scale_factor=self.scale_factor,mode=self.mode)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-23T07:55:11.422600Z","iopub.execute_input":"2025-08-23T07:55:11.422820Z","iopub.status.idle":"2025-08-23T07:55:11.427251Z","shell.execute_reply.started":"2025-08-23T07:55:11.422793Z","shell.execute_reply":"2025-08-23T07:55:11.426531Z"}},"outputs":[],"execution_count":47},{"id":"f45a8fcb","cell_type":"code","source":"class Neck(nn.Module):\n    def __init__(self,version):\n        super().__init__()\n        d,w,r=yolo_params(version)\n\n        self.up=Upsample() # no trainable parameters\n        self.c2f_1=C2f(in_channels=int(512*w*(1+r)), out_channels=int(512*w),num_bottlenecks=int(3*d),shortcut=False)\n        self.c2f_2=C2f(in_channels=int(768*w), out_channels=int(256*w),num_bottlenecks=int(3*d),shortcut=False)\n        self.c2f_3=C2f(in_channels=int(768*w), out_channels=int(512*w),num_bottlenecks=int(3*d),shortcut=False)\n        self.c2f_4=C2f(in_channels=int(512*w*(1+r)), out_channels=int(512*w*r),num_bottlenecks=int(3*d),shortcut=False)\n\n        self.cv_1=Conv(in_channels=int(256*w),out_channels=int(256*w),kernel_size=3,stride=2, padding=1)\n        self.cv_2=Conv(in_channels=int(512*w),out_channels=int(512*w),kernel_size=3,stride=2, padding=1)\n\n\n    def forward(self,x_res_1,x_res_2,x):    \n        # x_res_1,x_res_2,x = output of backbone\n        res_1=x              # for residual connection\n        \n        x=self.up(x)\n        x=torch.cat([x,x_res_2],dim=1)\n\n        res_2=self.c2f_1(x)  # for residual connection\n        \n        x=self.up(res_2)\n        x=torch.cat([x,x_res_1],dim=1)\n\n        out_1=self.c2f_2(x)\n\n        x=self.cv_1(out_1)\n\n        x=torch.cat([x,res_2],dim=1)\n        out_2=self.c2f_3(x)\n\n        x=self.cv_2(out_2)\n\n        x=torch.cat([x,res_1],dim=1)\n        out_3=self.c2f_4(x)\n\n        return out_1,out_2,out_3","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-23T07:55:11.427990Z","iopub.execute_input":"2025-08-23T07:55:11.428199Z","iopub.status.idle":"2025-08-23T07:55:11.444512Z","shell.execute_reply.started":"2025-08-23T07:55:11.428178Z","shell.execute_reply":"2025-08-23T07:55:11.443837Z"}},"outputs":[],"execution_count":48},{"id":"e8f1d070","cell_type":"code","source":"# DFL\nclass DFL(nn.Module):\n    def __init__(self,ch=16):\n        super().__init__()\n        \n        self.ch=ch\n        \n        self.conv=nn.Conv2d(in_channels=ch,out_channels=1,kernel_size=1,bias=False).requires_grad_(False)\n        \n        # initialize conv with [0,...,ch-1]\n        x=torch.arange(ch,dtype=torch.float).reshape(1,ch,1,1)\n        self.conv.weight.data[:]=torch.nn.Parameter(x) # DFL only has ch parameters\n\n    def forward(self,x):\n        # x must have num_channels = 4*ch: x=[bs,4*ch,c]\n        b,c,a=x.shape                           # c=4*ch\n        x=x.reshape(b,4,self.ch,a).transpose(1,2)  # [bs,ch,4,a]\n\n        # take softmax on channel dimension to get distribution probabilities\n        x=x.softmax(1)                          # [b,ch,4,a]\n        x=self.conv(x)                          # [b,1,4,a]\n        return x.reshape(b,4,a)                    # [b,4,a]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-23T07:55:11.445204Z","iopub.execute_input":"2025-08-23T07:55:11.445366Z","iopub.status.idle":"2025-08-23T07:55:11.454118Z","shell.execute_reply.started":"2025-08-23T07:55:11.445354Z","shell.execute_reply":"2025-08-23T07:55:11.453410Z"}},"outputs":[],"execution_count":49},{"id":"7f895d64","cell_type":"code","source":"class Head(nn.Module):\n    def __init__(self,version,ch=16,num_classes=5):\n\n        super().__init__()\n        self.ch=ch                          # dfl channels\n        self.coordinates=self.ch*4          # number of bounding box coordinates \n        self.nc=num_classes                 # 5 for custom dataset\n        self.no=self.coordinates+self.nc    # number of outputs per anchor box\n\n        self.stride = torch.tensor([8., 16., 32.])\n        \n        d,w,r=yolo_params(version=version)\n        \n        # for bounding boxes\n        self.box=nn.ModuleList([\n            nn.Sequential(Conv(int(256*w),self.coordinates,kernel_size=3,stride=1,padding=1),\n                          Conv(self.coordinates,self.coordinates,kernel_size=3,stride=1,padding=1),\n                          nn.Conv2d(self.coordinates,self.coordinates,kernel_size=1,stride=1)),\n\n            nn.Sequential(Conv(int(512*w),self.coordinates,kernel_size=3,stride=1,padding=1),\n                          Conv(self.coordinates,self.coordinates,kernel_size=3,stride=1,padding=1),\n                          nn.Conv2d(self.coordinates,self.coordinates,kernel_size=1,stride=1)),\n\n            nn.Sequential(Conv(int(512*w*r),self.coordinates,kernel_size=3,stride=1,padding=1),\n                          Conv(self.coordinates,self.coordinates,kernel_size=3,stride=1,padding=1),\n                          nn.Conv2d(self.coordinates,self.coordinates,kernel_size=1,stride=1))\n        ])\n\n        # for classification\n        self.cls=nn.ModuleList([\n            nn.Sequential(Conv(int(256*w),self.nc,kernel_size=3,stride=1,padding=1),\n                          Conv(self.nc,self.nc,kernel_size=3,stride=1,padding=1),\n                          nn.Conv2d(self.nc,self.nc,kernel_size=1,stride=1)),\n\n            nn.Sequential(Conv(int(512*w),self.nc,kernel_size=3,stride=1,padding=1),\n                          Conv(self.nc,self.nc,kernel_size=3,stride=1,padding=1),\n                          nn.Conv2d(self.nc,self.nc,kernel_size=1,stride=1)),\n\n            nn.Sequential(Conv(int(512*w*r),self.nc,kernel_size=3,stride=1,padding=1),\n                          Conv(self.nc,self.nc,kernel_size=3,stride=1,padding=1),\n                          nn.Conv2d(self.nc,self.nc,kernel_size=1,stride=1))\n        ])\n\n        # dfl\n        self.dfl=DFL()\n\n    def forward(self,x):\n        # x = output of Neck = list of 3 tensors with different resolution and different channel dim\n        #     x[0]=[bs, ch0, w0, h0], x[1]=[bs, ch1, w1, h1], x[2]=[bs,ch2, w2, h2] \n\n        for i in range(len(self.box)):       # detection head i\n            box=self.box[i](x[i])            # [bs,num_coordinates,w,h]\n            cls=self.cls[i](x[i])            # [bs,num_classes,w,h]\n            x[i]=torch.cat((box,cls),dim=1)  # [bs,num_coordinates+num_classes,w,h]\n\n        # in training, no dfl output\n        if self.training:\n            return x                         # [3,bs,num_coordinates+num_classes,w,h]\n        \n        # in inference time, dfl produces refined bounding box coordinates\n        anchors, strides = (i.transpose(0, 1) for i in self.make_anchors(x, self.stride))\n\n        # concatenate predictions from all detection layers\n        x = torch.cat([i.reshape(x[0].shape[0], self.no, -1) for i in x], dim=2) #[bs, 4*self.ch + self.nc, sum_i(h[i]w[i])]\n        \n        # split out predictions for box and cls\n        #           box=[bs,4×self.ch,sum_i(h[i]w[i])]\n        #           cls=[bs,self.nc,sum_i(h[i]w[i])]\n        box, cls = x.split(split_size=(4 * self.ch, self.nc), dim=1)\n\n\n        a, b = self.dfl(box).chunk(2, 1)  # a=b=[bs,2×self.ch,sum_i(h[i]w[i])]\n        a = anchors.unsqueeze(0) - a\n        b = anchors.unsqueeze(0) + b\n        box = torch.cat(tensors=((a + b) / 2, b - a), dim=1)\n        \n        return torch.cat(tensors=(box * strides, cls.sigmoid()), dim=1)\n\n\n    def make_anchors(self, x, strides, offset=0.5):\n        # x= list of feature maps: x=[x[0],...,x[N-1]], in our case N= num_detection_heads=3\n        #                          each having shape [bs,ch,w,h]\n        #    each feature map x[i] gives output[i] = w*h anchor coordinates + w*h stride values\n        \n        # strides = list of stride values indicating how much \n        #           the spatial resolution of the feature map is reduced compared to the original image\n\n        assert x is not None\n        anchor_tensor, stride_tensor = [], []\n        dtype, device = x[0].dtype, x[0].device\n        for i, stride in enumerate(strides):\n            _, _, h, w = x[i].shape\n            sx = torch.arange(end=w, device=device, dtype=dtype) + offset  # x coordinates of anchor centers\n            sy = torch.arange(end=h, device=device, dtype=dtype) + offset  # y coordinates of anchor centers\n            sy, sx = torch.meshgrid(sy, sx)                                # all anchor centers \n            anchor_tensor.append(torch.stack((sx, sy), -1).reshape(-1, 2))\n            stride_tensor.append(torch.full((h * w, 1), stride, dtype=dtype, device=device))\n        return torch.cat(anchor_tensor), torch.cat(stride_tensor)\n        ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-23T07:55:11.456001Z","iopub.execute_input":"2025-08-23T07:55:11.456245Z","iopub.status.idle":"2025-08-23T07:55:11.473651Z","shell.execute_reply.started":"2025-08-23T07:55:11.456223Z","shell.execute_reply":"2025-08-23T07:55:11.473063Z"}},"outputs":[],"execution_count":50},{"id":"07b79f85-a097-419b-8ef2-eb8a1d8f71bf","cell_type":"code","source":"import torch\n\n# fake feature maps (bs=1, ch=3)\n# ví dụ: 3 head detection tương ứng stride 8, 16, 32\nx = [\n    torch.zeros(1, 3, 80, 80),   # P3\n    torch.zeros(1, 3, 40, 40),   # P4\n    torch.zeros(1, 3, 20, 20)    # P5\n]\nstrides = [8, 16, 32]\n\ndef make_anchors(x, strides, offset=0.5):\n    anchor_tensor, stride_tensor = [], []\n    dtype, device = x[0].dtype, x[0].device\n    for i, stride in enumerate(strides):\n        _, _, h, w = x[i].shape\n        sx = torch.arange(end=w, device=device, dtype=dtype) + offset\n        sy = torch.arange(end=h, device=device, dtype=dtype) + offset\n        sy, sx = torch.meshgrid(sy, sx, indexing='ij')  # chú ý indexing\n        anchor_tensor.append(torch.stack((sx, sy), -1).reshape(-1, 2))\n        stride_tensor.append(torch.full((h * w, 1), stride, dtype=dtype, device=device))\n    return torch.cat(anchor_tensor), torch.cat(stride_tensor)\n\n# Test\nanchors, strides_out = make_anchors(x, strides)\n\nprint(\"Anchor tensor shape:\", anchors.shape)       # (8400, 2)\nprint(\"Stride tensor shape:\", strides_out.shape)   # (8400, 1)\n\n# In thử 5 anchor đầu tiên\nprint(\"First 5 anchors:\\n\", anchors[:5])\nprint(\"First 5 strides:\\n\", strides_out[:5].reshape(-1))\n\n# In thử cuối cùng (P5)\nprint(\"Last 5 anchors:\\n\", anchors[-5:])\nprint(\"Last 5 strides:\\n\", strides_out[-5:].reshape(-1))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-23T07:55:11.474455Z","iopub.execute_input":"2025-08-23T07:55:11.474722Z","iopub.status.idle":"2025-08-23T07:55:11.494773Z","shell.execute_reply.started":"2025-08-23T07:55:11.474707Z","shell.execute_reply":"2025-08-23T07:55:11.494173Z"}},"outputs":[{"name":"stdout","text":"Anchor tensor shape: torch.Size([8400, 2])\nStride tensor shape: torch.Size([8400, 1])\nFirst 5 anchors:\n tensor([[0.5000, 0.5000],\n        [1.5000, 0.5000],\n        [2.5000, 0.5000],\n        [3.5000, 0.5000],\n        [4.5000, 0.5000]])\nFirst 5 strides:\n tensor([8., 8., 8., 8., 8.])\nLast 5 anchors:\n tensor([[15.5000, 19.5000],\n        [16.5000, 19.5000],\n        [17.5000, 19.5000],\n        [18.5000, 19.5000],\n        [19.5000, 19.5000]])\nLast 5 strides:\n tensor([32., 32., 32., 32., 32.])\n","output_type":"stream"}],"execution_count":51},{"id":"0ee15e49","cell_type":"code","source":"class MyYolo(nn.Module):\n    def __init__(self, version, num_classes=5):\n        super().__init__()\n        self.backbone = Backbone(version=version)\n        self.neck = Neck(version=version)\n        self.head = Head(version=version, num_classes=num_classes)\n        self.nc = num_classes\n\n    def forward(self, x):\n        x = self.backbone(x)              # return out1, out2, out3\n        x = self.neck(x[0], x[1], x[2])   # return out_1, out_2, out_3\n        return self.head(list(x))\n\n\n# khởi tạo model với 5 class\nmodel = MyYolo(version='n', num_classes=5)\nprint(f\"{sum(p.numel() for p in model.parameters())/1e6:.2f} million parameters\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-23T07:55:11.495514Z","iopub.execute_input":"2025-08-23T07:55:11.496077Z","iopub.status.idle":"2025-08-23T07:55:11.544656Z","shell.execute_reply.started":"2025-08-23T07:55:11.496044Z","shell.execute_reply":"2025-08-23T07:55:11.543910Z"}},"outputs":[{"name":"stdout","text":"2.66 million parameters\n","output_type":"stream"}],"execution_count":52},{"id":"75f49c7b","cell_type":"markdown","source":"# UTIL","metadata":{}},{"id":"8c153daa-7de4-4ef1-9122-1c94942284d2","cell_type":"code","source":"# === DEBUG UTILS ===\nimport torch\nfrom contextlib import contextmanager\n\nDEBUG_ON = False          # Bật/tắt toàn cục\nDEBUG_MAX_ELEMS = 5      # In tối đa vài phần tử để đỡ rác\n\ndef tstats(name, t, mask=None):\n    if not DEBUG_ON: \n        return\n    try:\n        if mask is not None:\n            t = t[mask]\n        if t.numel() == 0:\n            print(f\"[{name}] empty tensor\")\n            return\n        t_det = t.detach()\n        print(f\"[{name}] shape={tuple(t.shape)} dtype={t.dtype} device={t.device} \"\n              f\"min={t_det.min().item():.6f} max={t_det.max().item():.6f} \"\n              f\"mean={t_det.float().mean().item():.6f} sum={t_det.float().sum().item():.6f} \"\n              f\"nnz={(t_det!=0).sum().item()}/{t_det.numel()}\")\n        # In vài phần tử đầu\n        flat = t_det.reshape(-1)\n        print(f\"  sample: {flat[:min(flat.numel(), DEBUG_MAX_ELEMS)].tolist()}\")\n        if torch.isnan(t_det).any() or torch.isinf(t_det).any():\n            print(f\"  WARN: {name} contains NaN/Inf\")\n    except Exception as e:\n        print(f\"[{name}] DEBUG ERROR: {e}\")\n\ndef tuniq(name, t):\n    if not DEBUG_ON: \n        return\n    try:\n        u = torch.unique(t)\n        print(f\"[{name}] unique({u.numel()}): {u[:min(u.numel(), DEBUG_MAX_ELEMS)].tolist()}\"\n              + (\" ...\" if u.numel() > DEBUG_MAX_ELEMS else \"\"))\n    except Exception as e:\n        print(f\"[{name}] unique() error: {e}\")\n\n@contextmanager\ndef debug_block(title):\n    if DEBUG_ON:\n        print(f\"\\n========== DEBUG: {title} ==========\")\n    yield\n    if DEBUG_ON:\n        print(f\"========== /DEBUG: {title} ==========\\n\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-23T07:55:11.545571Z","iopub.execute_input":"2025-08-23T07:55:11.545800Z","iopub.status.idle":"2025-08-23T07:55:11.553048Z","shell.execute_reply.started":"2025-08-23T07:55:11.545778Z","shell.execute_reply":"2025-08-23T07:55:11.552275Z"}},"outputs":[],"execution_count":53},{"id":"5056c9ef","cell_type":"code","source":"import copy\nimport random\nfrom time import time\n\nimport math\nimport numpy\nimport torch\nimport torchvision\nfrom torch.nn.functional import cross_entropy\n\ndef setup_seed():\n    \"\"\"\n    Setup random seed.\n    \"\"\"\n    random.seed(0)\n    numpy.random.seed(0)\n    torch.manual_seed(0)\n    torch.backends.cudnn.benchmark = False\n    torch.backends.cudnn.deterministic = True\n\n\ndef setup_multi_processes():\n    \"\"\"\n    Setup multi-processing environment variables.\n    \"\"\"\n    import cv2\n    from os import environ\n    from platform import system\n\n    # set multiprocess start method as `fork` to speed up the training\n    if system() != 'Windows':\n        torch.multiprocessing.set_start_method('fork', force=True)\n\n    # disable opencv multithreading to avoid system being overloaded\n    cv2.setNumThreads(0)\n\n    # setup OMP threads\n    if 'OMP_NUM_THREADS' not in environ:\n        environ['OMP_NUM_THREADS'] = '1'\n\n    # setup MKL threads\n    if 'MKL_NUM_THREADS' not in environ:\n        environ['MKL_NUM_THREADS'] = '1'\n\n\ndef export_onnx(args):\n    import onnx  # noqa\n\n    inputs = ['images']\n    outputs = ['outputs']\n    dynamic = {'outputs': {0: 'batch', 1: 'anchors'}}\n\n    m = torch.load('./weights/best.pt')['model'].float()\n    x = torch.zeros((1, 3, args.input_size, args.input_size))\n\n    torch.onnx.export(m.cpu(), x.cpu(),\n                      f='./weights/best.onnx',\n                      verbose=False,\n                      opset_version=12,\n                      # WARNING: DNN inference with torch>=1.12 may require do_constant_folding=False\n                      do_constant_folding=True,\n                      input_names=inputs,\n                      output_names=outputs,\n                      dynamic_axes=dynamic or None)\n\n    # Checks\n    model_onnx = onnx.load('./weights/best.onnx')  # load onnx model\n    onnx.checker.check_model(model_onnx)  # check onnx model\n\n    onnx.save(model_onnx, './weights/best.onnx')\n    # Inference example\n    # https://github.com/ultralytics/ultralytics/blob/main/ultralytics/nn/autobackend.py\n\n\n# def wh2xy(x):\n#     y = x.clone() if isinstance(x, torch.Tensor) else numpy.copy(x)\n#     y[:, 0] = x[:, 0] - x[:, 2] / 2  # top left x\n#     y[:, 1] = x[:, 1] - x[:, 3] / 2  # top left y\n#     y[:, 2] = x[:, 0] + x[:, 2] / 2  # bottom right x\n#     y[:, 3] = x[:, 1] + x[:, 3] / 2  # bottom right y\n#     return y\n\ndef wh2xy(x, w=640, h=640, pad_w=0, pad_h=0):\n    # Convert nx4 boxes\n    # from [x, y, w, h] normalized to [x1, y1, x2, y2] where xy1=top-left, xy2=bottom-right\n    y = numpy.copy(x)\n    y[:, 0] = w * (x[:, 0] - x[:, 2] / 2) + pad_w  # top left x\n    y[:, 1] = h * (x[:, 1] - x[:, 3] / 2) + pad_h  # top left y\n    y[:, 2] = w * (x[:, 0] + x[:, 2] / 2) + pad_w  # bottom right x\n    y[:, 3] = h * (x[:, 1] + x[:, 3] / 2) + pad_h  # bottom right y\n    return y\n\ndef make_anchors(x, strides, offset=0.5):\n    assert x is not None\n    anchor_tensor, stride_tensor = [], []\n    dtype, device = x[0].dtype, x[0].device\n    for i, stride in enumerate(strides):\n        _, _, h, w = x[i].shape\n        sx = torch.arange(end=w, device=device, dtype=dtype) + offset  # shift x\n        sy = torch.arange(end=h, device=device, dtype=dtype) + offset  # shift y\n        sy, sx = torch.meshgrid(sy, sx, indexing='ij')\n        anchor_tensor.append(torch.stack((sx, sy), -1).reshape(-1, 2))\n        stride_tensor.append(torch.full((h * w, 1), stride, dtype=dtype, device=device))\n    return torch.cat(anchor_tensor), torch.cat(stride_tensor)\n\n\ndef compute_metric(output, target, iou_v):\n    # intersection(N,M) = (rb(N,M,2) - lt(N,M,2)).clamp(0).prod(2)\n    (a1, a2) = target[:, 1:].unsqueeze(1).chunk(2, 2)\n    (b1, b2) = output[:, :4].unsqueeze(0).chunk(2, 2)\n    intersection = (torch.min(a2, b2) - torch.max(a1, b1)).clamp(0).prod(2)\n    # IoU = intersection / (area1 + area2 - intersection)\n    iou = intersection / ((a2 - a1).prod(2) + (b2 - b1).prod(2) - intersection + 1e-7)\n\n    correct = numpy.zeros((output.shape[0], iou_v.shape[0]))\n    correct = correct.astype(bool)\n    for i in range(len(iou_v)):\n        # IoU > threshold and classes match\n        x = torch.where((iou >= iou_v[i]) & (target[:, 0:1] == output[:, 5]))\n        if x[0].shape[0]:\n            matches = torch.cat((torch.stack(x, 1),\n                                 iou[x[0], x[1]][:, None]), 1).cpu().numpy()  # [label, detect, iou]\n            if x[0].shape[0] > 1:\n                matches = matches[matches[:, 2].argsort()[::-1]]\n                matches = matches[numpy.unique(matches[:, 1], return_index=True)[1]]\n                matches = matches[numpy.unique(matches[:, 0], return_index=True)[1]]\n            correct[matches[:, 1].astype(int), i] = True\n    return torch.tensor(correct, dtype=torch.bool, device=output.device)\n\n\ndef non_max_suppression(outputs, confidence_threshold=0.001, iou_threshold=0.7):\n    max_wh = 7680\n    max_det = 300\n    max_nms = 30000\n\n    bs = outputs.shape[0]  # batch size\n    nc = outputs.shape[1] - 4  # number of classes\n    xc = outputs[:, 4:4 + nc].amax(1) > confidence_threshold  # candidates\n\n    # Settings\n    start = time()\n    limit = 0.5 + 0.05 * bs  # seconds to quit after\n    output = [torch.zeros((0, 6), device=outputs.device)] * bs\n    for index, x in enumerate(outputs):  # image index, image inference\n        x = x.transpose(0, -1)[xc[index]]  # confidence\n\n        # If none remain process next image\n        if not x.shape[0]:\n            continue\n\n        # matrix nx6 (box, confidence, cls)\n        box, cls = x.split((4, nc), 1)\n        box = wh2xy(box)  # (cx, cy, w, h) to (x1, y1, x2, y2)\n        if nc > 1:\n            i, j = (cls > confidence_threshold).nonzero(as_tuple=False).T\n            x = torch.cat((box[i], x[i, 4 + j, None], j[:, None].float()), 1)\n        else:  # best class only\n            conf, j = cls.max(1, keepdim=True)\n            x = torch.cat((box, conf, j.float()), 1)[conf.view(-1) > confidence_threshold]  #Không\n\n        # Check shape\n        n = x.shape[0]  # number of boxes\n        if not n:  # no boxes\n            continue\n        x = x[x[:, 4].argsort(descending=True)[:max_nms]]  # sort by confidence and remove excess boxes\n\n        # Batched NMS\n        c = x[:, 5:6] * max_wh  # classes\n        boxes, scores = x[:, :4] + c, x[:, 4]  # boxes, scores\n        indices = torchvision.ops.nms(boxes, scores, iou_threshold)  # NMS\n        indices = indices[:max_det]  # limit detections\n\n        output[index] = x[indices]\n        if (time() - start) > limit:\n            break  # time limit exceeded\n\n    return output\n\n\ndef smooth(y, f=0.05):\n    # Box filter of fraction f\n    nf = round(len(y) * f * 2) // 2 + 1  # number of filter elements (must be odd)\n    p = numpy.ones(nf // 2)  # ones padding\n    yp = numpy.concatenate((p * y[0], y, p * y[-1]), 0)  # y padded\n    return numpy.convolve(yp, numpy.ones(nf) / nf, mode='valid')  # y-smoothed\n\n\ndef compute_ap(tp, conf, pred_cls, target_cls, eps=1e-16):\n    \"\"\"\n    Compute the average precision, given the recall and precision curves.\n    Source: https://github.com/rafaelpadilla/Object-Detection-Metrics.\n    # Arguments\n        tp:  True positives (nparray, nx1 or nx10).\n        conf:  Object-ness value from 0-1 (nparray).\n        pred_cls:  Predicted object classes (nparray).\n        target_cls:  True object classes (nparray).\n    # Returns\n        The average precision\n    \"\"\"\n    # Sort by object-ness\n    i = numpy.argsort(-conf)\n    tp, conf, pred_cls = tp[i], conf[i], pred_cls[i]\n\n    # Find unique classes\n    unique_classes, nt = numpy.unique(target_cls, return_counts=True)\n    nc = unique_classes.shape[0]  # number of classes, number of detections\n\n    # Create Precision-Recall curve and compute AP for each class\n    p = numpy.zeros((nc, 1000))\n    r = numpy.zeros((nc, 1000))\n    ap = numpy.zeros((nc, tp.shape[1]))\n    px, py = numpy.linspace(0, 1, 1000), []  # for plotting\n    for ci, c in enumerate(unique_classes):\n        i = pred_cls == c\n        nl = nt[ci]  # number of labels\n        no = i.sum()  # number of outputs\n        if no == 0 or nl == 0:\n            continue\n\n        # Accumulate FPs and TPs\n        fpc = (1 - tp[i]).cumsum(0)\n        tpc = tp[i].cumsum(0)\n\n        # Recall\n        recall = tpc / (nl + eps)  # recall curve\n        # negative x, xp because xp decreases\n        r[ci] = numpy.interp(-px, -conf[i], recall[:, 0], left=0)\n\n        # Precision\n        precision = tpc / (tpc + fpc)  # precision curve\n        p[ci] = numpy.interp(-px, -conf[i], precision[:, 0], left=1)  # p at pr_score\n\n        # AP from recall-precision curve\n        for j in range(tp.shape[1]):\n            m_rec = numpy.concatenate(([0.0], recall[:, j], [1.0]))\n            m_pre = numpy.concatenate(([1.0], precision[:, j], [0.0]))\n\n            # Compute the precision envelope\n            m_pre = numpy.flip(numpy.maximum.accumulate(numpy.flip(m_pre)))\n\n            # Integrate area under curve\n            x = numpy.linspace(0, 1, 101)  # 101-point interp (COCO)\n            ap[ci, j] = numpy.trapz(numpy.interp(x, m_rec, m_pre), x)  # integrate\n\n    # Compute F1 (harmonic mean of precision and recall)\n    f1 = 2 * p * r / (p + r + eps)\n\n    i = smooth(f1.mean(0), 0.1).argmax()  # max F1 index\n    p, r, f1 = p[:, i], r[:, i], f1[:, i]\n    tp = (r * nt).round()  # true positives\n    fp = (tp / (p + eps) - tp).round()  # false positives\n    ap50, ap = ap[:, 0], ap.mean(1)  # AP@0.5, AP@0.5:0.95\n    m_pre, m_rec = p.mean(), r.mean()\n    map50, mean_ap = ap50.mean(), ap.mean()\n    return tp, fp, m_pre, m_rec, map50, mean_ap\n\n\ndef compute_iou(box1, box2, eps=1e-7):\n    # Returns Intersection over Union (IoU) of box1(1,4) to box2(n,4)\n\n    # Get the coordinates of bounding boxes\n    b1_x1, b1_y1, b1_x2, b1_y2 = box1.chunk(4, -1)\n    b2_x1, b2_y1, b2_x2, b2_y2 = box2.chunk(4, -1)\n    w1, h1 = b1_x2 - b1_x1, b1_y2 - b1_y1 + eps\n    w2, h2 = b2_x2 - b2_x1, b2_y2 - b2_y1 + eps\n\n    # Intersection area\n    inter = (b1_x2.minimum(b2_x2) - b1_x1.maximum(b2_x1)).clamp(0) * \\\n            (b1_y2.minimum(b2_y2) - b1_y1.maximum(b2_y1)).clamp(0)\n\n    # Union Area\n    union = w1 * h1 + w2 * h2 - inter + eps\n\n    # IoU\n    iou = inter / union\n    cw = b1_x2.maximum(b2_x2) - b1_x1.minimum(b2_x1)  # convex (smallest enclosing box) width\n    ch = b1_y2.maximum(b2_y2) - b1_y1.minimum(b2_y1)  # convex height\n    c2 = cw ** 2 + ch ** 2 + eps  # convex diagonal squared\n    rho2 = ((b2_x1 + b2_x2 - b1_x1 - b1_x2) ** 2 + (b2_y1 + b2_y2 - b1_y1 - b1_y2) ** 2) / 4  # center dist ** 2\n    # https://github.com/Zzh-tju/DIoU-SSD-pytorch/blob/master/utils/box/box_utils.py#L47\n    v = (4 / math.pi ** 2) * (torch.atan(w2 / h2) - torch.atan(w1 / h1)).pow(2)\n    with torch.no_grad():\n        alpha = v / (v - iou + (1 + eps))\n\n    return iou - (rho2 / c2 + v * alpha)  # CIoU\n\n\ndef strip_optimizer(filename):\n    x = torch.load(filename, map_location=\"cpu\")\n    x['model'].half()  # to FP16\n    for p in x['model'].parameters():\n        p.requires_grad = False\n    torch.save(x, f=filename)\n\n\ndef clip_gradients(model, max_norm=10.0):\n    parameters = model.parameters()\n    torch.nn.utils.clip_grad_norm_(parameters, max_norm=max_norm)\n\n\ndef load_weight(model, ckpt):\n    dst = model.state_dict()\n    src = torch.load(ckpt)['model'].float().cpu()\n\n    ckpt = {}\n    for k, v in src.state_dict().items():\n        if k in dst and v.shape == dst[k].shape:\n            ckpt[k] = v\n\n    model.load_state_dict(state_dict=ckpt, strict=False)\n    return model\n\n\ndef set_params(model, decay):\n    p1 = []\n    p2 = []\n    for name, param in model.named_parameters():\n        if not param.requires_grad:\n            continue\n        if param.ndim <= 1 or name.endswith(\".bias\"):\n            p1.append(param)\n        else:\n            p2.append(param)\n    return [{'params': p1, 'weight_decay': 0.00},\n            {'params': p2, 'weight_decay': decay}]\n\n\ndef plot_lr(args, optimizer, scheduler, num_steps):\n    from matplotlib import pyplot\n\n    optimizer = copy.copy(optimizer)\n    scheduler = copy.copy(scheduler)\n\n    y = []\n    for epoch in range(args.epochs):\n        for i in range(num_steps):\n            step = i + num_steps * epoch\n            scheduler.step(step, optimizer)\n            y.append(optimizer.param_groups[0]['lr'])\n    print(y[0])\n    print(y[-1])\n    pyplot.plot(y, '.-', label='LR')\n    pyplot.xlabel('step')\n    pyplot.ylabel('LR')\n    pyplot.grid()\n    pyplot.xlim(0, args.epochs * num_steps)\n    pyplot.ylim(0)\n    pyplot.savefig('./weights/lr.png', dpi=200)\n    pyplot.close()\n\n\nclass CosineLR:\n    def __init__(self, args, params, num_steps):\n        max_lr = params['max_lr']\n        min_lr = params['min_lr']\n\n        warmup_steps = int(max(params['warmup_epochs'] * num_steps, 100))\n        decay_steps = int(args.epochs * num_steps - warmup_steps)\n\n        warmup_lr = numpy.linspace(min_lr, max_lr, int(warmup_steps))\n\n        decay_lr = []\n        for step in range(1, decay_steps + 1):\n            alpha = math.cos(math.pi * step / decay_steps)\n            decay_lr.append(min_lr + 0.5 * (max_lr - min_lr) * (1 + alpha))\n\n        self.total_lr = numpy.concatenate((warmup_lr, decay_lr))\n\n    def step(self, step, optimizer):\n        for param_group in optimizer.param_groups:\n            param_group['lr'] = self.total_lr[step]\n\n\nclass LinearLR:\n    def __init__(self, args, params, num_steps):\n        max_lr = params['max_lr']\n        min_lr = params['min_lr']\n\n        warmup_steps = int(max(params['warmup_epochs'] * num_steps, 100))\n        decay_steps = int(args.epochs * num_steps - warmup_steps)\n\n        warmup_lr = numpy.linspace(min_lr, max_lr, int(warmup_steps), endpoint=False)\n        decay_lr = numpy.linspace(max_lr, min_lr, decay_steps)\n\n        self.total_lr = numpy.concatenate((warmup_lr, decay_lr))\n\n    def step(self, step, optimizer):\n        for param_group in optimizer.param_groups:\n            param_group['lr'] = self.total_lr[step]\n\n\nclass EMA:\n    \"\"\"\n    Updated Exponential Moving Average (EMA) from https://github.com/rwightman/pytorch-image-models\n    Keeps a moving average of everything in the model state_dict (parameters and buffers)\n    For EMA details see https://www.tensorflow.org/api_docs/python/tf/train/ExponentialMovingAverage\n    \"\"\"\n\n    def __init__(self, model, decay=0.9999, tau=2000, updates=0):\n        # Create EMA\n        self.ema = copy.deepcopy(model).eval()  # FP32 EMA\n        self.updates = updates  # number of EMA updates\n        # decay exponential ramp (to help early epochs)\n        self.decay = lambda x: decay * (1 - math.exp(-x / tau))\n        for p in self.ema.parameters():\n            p.requires_grad_(False)\n\n    def update(self, model):\n        if hasattr(model, 'module'):\n            model = model.module\n        # Update EMA parameters\n        with torch.no_grad():\n            self.updates += 1\n            d = self.decay(self.updates)\n\n            msd = model.state_dict()  # model state_dict\n            for k, v in self.ema.state_dict().items():\n                if v.dtype.is_floating_point:\n                    v *= d\n                    v += (1 - d) * msd[k].detach()\n\n\nclass AverageMeter:\n    def __init__(self):\n        self.num = 0\n        self.sum = 0\n        self.avg = 0\n\n    def update(self, v, n):\n        if not math.isnan(float(v)):\n            self.num = self.num + n\n            self.sum = self.sum + v * n\n            self.avg = self.sum / self.num\n\n\nclass Assigner(torch.nn.Module):\n    def __init__(self, nc=5, top_k=13, alpha=1.0, beta=6.0, eps=1E-9):\n        super().__init__()\n        self.top_k = top_k\n        self.nc = nc\n        self.alpha = alpha\n        self.beta = beta\n        self.eps = eps\n\n    @torch.no_grad()\n    def forward(self, pd_scores, pd_bboxes, anc_points, gt_labels, gt_bboxes, mask_gt):\n        # with debug_block(\"Assigner.forward / inputs\"):\n        #     print(f\"pd_scores: {tuple(pd_scores.shape)}  (B, A, C)\")\n        #     print(f\"pd_bboxes: {tuple(pd_bboxes.shape)}  (B, A, 4)\")\n        #     print(f\"anc_points: {tuple(anc_points.shape)} (A, 2)\")\n        #     print(f\"gt_labels: {tuple(gt_labels.shape)}  (B, M, 1)\")\n        #     print(f\"gt_bboxes: {tuple(gt_bboxes.shape)}  (B, M, 4)\")\n        #     print(f\"mask_gt sum: {mask_gt.sum().item()}\")\n        #     tuniq(\"gt_labels uniq\", gt_labels.reshape(-1))\n        \n        batch_size = pd_scores.size(0)\n        num_max_boxes = gt_bboxes.size(1)\n\n        if num_max_boxes == 0:\n            device = gt_bboxes.device\n            print(\"Assigner: num_max_boxes==0 -> return zeros\")\n            return (torch.zeros_like(pd_bboxes).to(device),\n                    torch.zeros_like(pd_scores).to(device),\n                    torch.zeros_like(pd_scores[..., 0]).to(device))\n\n        num_anchors = anc_points.shape[0]\n        shape = gt_bboxes.shape\n        lt, rb = gt_bboxes.reshape(-1, 1, 4).chunk(2, 2)\n        mask_in_gts = torch.cat((anc_points[None] - lt, rb - anc_points[None]), dim=2)\n        mask_in_gts = mask_in_gts.reshape(shape[0], shape[1], num_anchors, -1).amin(3).gt_(self.eps)\n\n        \n        # with debug_block(\"in GT mask\"):\n        #     print(\"mask_in_gts sum:\", mask_in_gts.sum().item())\n        \n        na = pd_bboxes.shape[-2]\n        gt_mask = (mask_in_gts * mask_gt).bool()  # b, max_num_obj, h*w\n        overlaps = torch.zeros([batch_size, num_max_boxes, na], dtype=pd_bboxes.dtype, device=pd_bboxes.device)\n        bbox_scores = torch.zeros([batch_size, num_max_boxes, na], dtype=pd_scores.dtype, device=pd_scores.device)\n\n        ind = torch.zeros([2, batch_size, num_max_boxes], dtype=torch.long)  # 2, b, max_num_obj\n        ind[0] = torch.arange(end=batch_size).reshape(-1, 1).expand(-1, num_max_boxes)  # b, max_num_obj\n        ind[1] = gt_labels.squeeze(-1)  # b, max_num_obj\n        bbox_scores[gt_mask] = pd_scores[ind[0], :, ind[1]][gt_mask]  # b, max_num_obj, h*w\n\n        pd_boxes = pd_bboxes.unsqueeze(1).expand(-1, num_max_boxes, -1, -1)[gt_mask]\n        gt_boxes = gt_bboxes.unsqueeze(2).expand(-1, -1, na, -1)[gt_mask]\n        overlaps[gt_mask] = compute_iou(gt_boxes, pd_boxes).squeeze(-1).clamp_(0)\n\n        # with debug_block(\"overlaps & scores\"):\n        #     tstats(\"bbox_scores (selected)\", bbox_scores[gt_mask])\n        #     tstats(\"overlaps (selected)\", overlaps[gt_mask])\n        \n\n        align_metric = bbox_scores.pow(self.alpha) * overlaps.pow(self.beta)\n        \n        # with debug_block(\"align_metric\"):\n        #     tstats(\"align_metric(all)\", align_metric)\n        #     tstats(\"align_metric(selected)\", align_metric[gt_mask])\n\n        top_k_mask = mask_gt.expand(-1, -1, self.top_k).bool()\n        top_k_metrics, top_k_indices = torch.topk(align_metric, self.top_k, dim=-1, largest=True)\n        if top_k_mask is None:\n            top_k_mask = (top_k_metrics.max(-1, keepdim=True)[0] > self.eps).expand_as(top_k_indices)\n        top_k_indices.masked_fill_(~top_k_mask, 0)\n\n        # with debug_block(\"top-k\"):\n        #     tstats(\"top_k_metrics\", top_k_metrics)\n        #     tstats(\"top_k_indices\", top_k_indices)\n\n        mask_top_k = torch.zeros(align_metric.shape, dtype=torch.int8, device=top_k_indices.device)\n        ones = torch.ones_like(top_k_indices[:, :, :1], dtype=torch.int8, device=top_k_indices.device)\n        for k in range(self.top_k):\n            mask_top_k.scatter_add_(-1, top_k_indices[:, :, k:k + 1], ones)\n        mask_top_k.masked_fill_(mask_top_k > 1, 0)\n        mask_top_k = mask_top_k.to(align_metric.dtype)\n        mask_pos = mask_top_k * mask_in_gts * mask_gt\n\n        # with debug_block(\"positive mask\"):\n        #     print(\"mask_pos sum:\", mask_pos.sum().item())\n        #     print(\"pos per-gt:\", mask_pos.sum(-1)[mask_gt.squeeze(-1).bool()].reshape(-1).tolist()[:20])\n        \n        fg_mask = mask_pos.sum(-2)\n        if fg_mask.max() > 1:\n            mask_multi_gts = (fg_mask.unsqueeze(1) > 1).expand(-1, num_max_boxes, -1)\n            max_overlaps_idx = overlaps.argmax(1)\n\n            is_max_overlaps = torch.zeros(mask_pos.shape, dtype=mask_pos.dtype, device=mask_pos.device)\n            is_max_overlaps.scatter_(1, max_overlaps_idx.unsqueeze(1), 1)\n\n            mask_pos = torch.where(mask_multi_gts, is_max_overlaps, mask_pos).float()\n            fg_mask = mask_pos.sum(-2)\n        target_gt_idx = mask_pos.argmax(-2)\n\n        # Assigned target\n        index = torch.arange(end=batch_size, dtype=torch.int64, device=gt_labels.device)[..., None]\n        target_index = target_gt_idx + index * num_max_boxes\n        target_labels = gt_labels.long().flatten()[target_index]\n\n        target_bboxes = gt_bboxes.reshape(-1, gt_bboxes.shape[-1])[target_index]\n\n        # SỬA\n        # labels hợp lệ?\n        assert (target_labels >= 0).all() and (target_labels < self.nc).all(), \"Assigned labels out of range\"\n\n        \n        # Assigned target scores\n        target_labels.clamp_(min=0, max=self.nc - 1)\n\n        target_scores = torch.zeros((target_labels.shape[0], target_labels.shape[1], self.nc),\n                                    #dtype=torch.int64,  #SỬA\n                                    dtype=torch.float32,\n                                    device=target_labels.device)\n        target_scores.scatter_(2, target_labels.unsqueeze(-1), 1)\n\n        fg_scores_mask = fg_mask[:, :, None].repeat(1, 1, self.nc)\n        target_scores = torch.where(fg_scores_mask > 0, target_scores, 0)\n\n        # Normalize\n        align_metric *= mask_pos\n        pos_align_metrics = align_metric.amax(dim=-1, keepdim=True)\n        pos_overlaps = (overlaps * mask_pos).amax(dim=-1, keepdim=True)\n        norm_align_metric = (align_metric * pos_overlaps / (pos_align_metrics + self.eps)).amax(-2).unsqueeze(-1)\n        target_scores = target_scores * norm_align_metric\n\n        \n        # with debug_block(\"assigner outputs\"):\n        #     tstats(\"fg_mask\", fg_mask)\n        #     tstats(\"target_labels\", target_labels)\n        #     tstats(\"norm_align_metric\", norm_align_metric)\n        #     print(\"target_scores > 0:\", (target_scores > 0).sum().item())\n        \n        return target_bboxes, target_scores, fg_mask.bool()\n\n\nclass QFL(torch.nn.Module):\n    def __init__(self, beta=2.0):\n        super().__init__()\n        self.beta = beta\n        self.bce_loss = torch.nn.BCEWithLogitsLoss(reduction='none')\n\n    def forward(self, outputs, targets):\n        bce_loss = self.bce_loss(outputs, targets)\n        return torch.pow(torch.abs(targets - outputs.sigmoid()), self.beta) * bce_loss\n\n\nclass VFL(torch.nn.Module):\n    def __init__(self, alpha=0.75, gamma=2.00, iou_weighted=True):\n        super().__init__()\n        assert alpha >= 0.0\n        self.alpha = alpha\n        self.gamma = gamma\n        self.iou_weighted = iou_weighted\n        self.bce_loss = torch.nn.BCEWithLogitsLoss(reduction='none')\n\n    def forward(self, outputs, targets):\n        assert outputs.size() == targets.size()\n        targets = targets.type_as(outputs)\n\n        if self.iou_weighted:\n            focal_weight = targets * (targets > 0.0).float() + \\\n                           self.alpha * (outputs.sigmoid() - targets).abs().pow(self.gamma) * \\\n                           (targets <= 0.0).float()\n\n        else:\n            focal_weight = (targets > 0.0).float() + \\\n                           self.alpha * (outputs.sigmoid() - targets).abs().pow(self.gamma) * \\\n                           (targets <= 0.0).float()\n\n        return self.bce_loss(outputs, targets) * focal_weight\n\n\nclass BoxLoss(torch.nn.Module):\n    def __init__(self, dfl_ch):\n        super().__init__()\n        self.dfl_ch = dfl_ch\n\n    def forward(self, pred_dist, pred_bboxes, anchor_points, target_bboxes, target_scores, target_scores_sum, fg_mask):\n        # with debug_block(\"BoxLoss.forward / inputs\"):\n        #     print(\"fg_mask sum:\", fg_mask.sum().item())\n        #     tstats(\"target_scores sum per pos\", torch.masked_select(target_scores.sum(-1), fg_mask))\n        #     tstats(\"pred_bboxes(pos)\", pred_bboxes[fg_mask])\n        #     tstats(\"target_bboxes(pos)\", target_bboxes[fg_mask])\n            \n        # IoU loss\n        weight = torch.masked_select(target_scores.sum(-1), fg_mask).unsqueeze(-1)\n        iou = compute_iou(pred_bboxes[fg_mask], target_bboxes[fg_mask])\n        loss_box = ((1.0 - iou) * weight).sum() / target_scores_sum\n\n        # with debug_block(\"IoU part\"):\n        #     tstats(\"IoU(pos)\", iou)\n        #     print(\"loss_box(partial):\", float(loss_box.detach().cpu()))\n\n        # DFL loss\n        a, b = target_bboxes.chunk(2, -1)\n\n\n        # SỬA: broadcast anchor_points theo batch để khớp fg_mask\n        batch_size = fg_mask.shape[0]                          \n        anchors_batched = anchor_points.unsqueeze(0).expand(batch_size, -1, -1)  # [B, num_anchors, 2]\n\n        # SỬA: dùng anchors_batched thay cho anchor_points\n        target = torch.cat((anchors_batched - a, b - anchors_batched), -1)\n        target = target.clamp(0, self.dfl_ch - 0.01)\n\n        # with debug_block(\"DFL target build\"):\n        #     tstats(\"anchor_points(pos)\", anchors_batched[fg_mask])  # SỬA: dùng anchors_batched\n        #     tstats(\"target distances(pos)\", target[fg_mask])\n        #     print(\"dfl_ch:\", self.dfl_ch)\n        \n        loss_dfl = self.df_loss(pred_dist[fg_mask].reshape(-1, self.dfl_ch + 1), target[fg_mask])\n        loss_dfl = (loss_dfl * weight).sum() / target_scores_sum\n\n\n        # with debug_block(\"DFL final\"):\n        #     tstats(\"loss_dfl(per-pos)\", loss_dfl.unsqueeze(0))\n        #     print(\"loss_dfl:\", float(loss_dfl.detach().cpu()))\n        \n        return loss_box, loss_dfl\n\n    @staticmethod\n    def df_loss(pred_dist, target):\n        # Distribution Focal Loss (DFL)\n        # https://ieeexplore.ieee.org/document/9792391\n        tl = target.long()  # target left\n        tr = tl + 1  # target right\n        wl = tr - target  # weight left\n        wr = 1 - wl  # weight right\n        left_loss = cross_entropy(pred_dist, tl.reshape(-1), reduction='none').reshape(tl.shape)\n        right_loss = cross_entropy(pred_dist, tr.reshape(-1), reduction='none').reshape(tl.shape)\n        return (left_loss * wl + right_loss * wr).mean(-1, keepdim=True)\n\n\nclass ComputeLoss:\n    def __init__(self, model, params):\n        if hasattr(model, 'module'):\n            model = model.module\n\n        device = next(model.parameters()).device\n\n        m = model.head  # Head() module\n\n        self.params = params\n        self.stride = m.stride\n        self.nc = m.nc\n        self.no = m.no\n        self.reg_max = m.ch\n        self.device = device\n\n        self.box_loss = BoxLoss(m.ch - 1).to(device)\n        self.cls_loss = torch.nn.BCEWithLogitsLoss(reduction='none')\n        self.assigner = Assigner(nc=self.nc, top_k=10, alpha=0.5, beta=6.0)\n\n        self.project = torch.arange(m.ch, dtype=torch.float, device=device)\n\n    # def box_decode(self, anchor_points, pred_dist):\n    #     b, a, c = pred_dist.shape\n    #     pred_dist = pred_dist.reshape(b, a, 4, c // 4)\n    #     pred_dist = pred_dist.softmax(3)\n    #     pred_dist = pred_dist.matmul(self.project.type(pred_dist.dtype))\n    #     lt, rb = pred_dist.chunk(2, -1)\n    #     x1y1 = anchor_points - lt\n    #     x2y2 = anchor_points + rb\n    #     return torch.cat(tensors=(x1y1, x2y2), dim=-1)\n\n    # def box_decode(self, anchor_points, pred_dist):\n    #     b, a, c = pred_dist.shape\n    #     pred_dist = pred_dist.reshape(b, a, 4, c // 4)\n    #     pred_dist = pred_dist.softmax(3)\n    #     # SỬA: Ensure self.project is on the same device as pred_dist\n    #     project = self.project.to(pred_dist.device).type(pred_dist.dtype)\n    #     pred_dist = pred_dist.matmul(project)\n    #     lt, rb = pred_dist.chunk(2, -1)\n    #     x1y1 = anchor_points - lt\n    #     x2y2 = anchor_points + rb\n    #     return torch.cat(tensors=(x1y1, x2y2), dim=-1)\n\n    def box_decode(self, anchor_points, pred_dist):\n        b, a, c = pred_dist.shape\n        pred_dist = pred_dist.reshape(b, a, 4, c // 4)\n        pred_dist = pred_dist.softmax(3)\n    \n        # Ensure all tensors on the same device\n        device = pred_dist.device\n        anchor_points = anchor_points.to(device)\n        project = self.project.to(device).type(pred_dist.dtype)\n    \n        pred_dist = pred_dist.matmul(project)\n        lt, rb = pred_dist.chunk(2, -1)\n        x1y1 = anchor_points - lt\n        x2y2 = anchor_points + rb\n    \n        return torch.cat((x1y1, x2y2), dim=-1)\n\n    # def __call__(self, outputs, targets):\n    #     #*****************************\n    #     # with debug_block(\"ComputeLoss.__call__ / inputs\"):\n    #     #     # Kiểm tra outputs\n    #     #     print(\"num.feature levels:\", len(outputs))\n    #     #     for li, o in enumerate(outputs):\n    #     #         print(f\"  L{li} shape={tuple(o.shape)}\")  # (B, no, H, W)\n    \n    #     #     # Kiểm tra targets dict\n    #     #     print(\"targets keys:\", list(targets.keys()))\n    #     #     tstats(\"targets['idx']\", targets['idx'])\n    #     #     tstats(\"targets['cls']\", targets['cls'])\n    #     #     tstats(\"targets['box']\", targets['box'])\n    #     #     if 'cls' in targets:\n    #     #         tuniq(\"targets['cls'] uniq\", targets['cls'])\n    #     #*****************************************\n\n\n\n\n        \n    #     x = torch.cat([i.reshape(outputs[0].shape[0], self.no, -1) for i in outputs], dim=2)\n    #     pred_distri, pred_scores = x.split(split_size=(self.reg_max * 4, self.nc), dim=1)\n\n    #     pred_scores = pred_scores.permute(0, 2, 1).contiguous()\n    #     pred_distri = pred_distri.permute(0, 2, 1).contiguous()\n\n\n\n        \n    #     #***********************************     \n    #     # with debug_block(\"pred tensors\"):\n    #     #     tstats(\"pred_scores(logits)\", pred_scores)\n    #     #     tstats(\"pred_scores(sigmoid)\", pred_scores.sigmoid())\n    #     #     tstats(\"pred_distri\", pred_distri)\n    #     #*********************************\n\n\n\n        \n    #     data_type = pred_scores.dtype\n    #     batch_size = pred_scores.shape[0]\n    #     input_size = torch.tensor(outputs[0].shape[2:], device=self.device, dtype=data_type) * self.stride[0]\n    #     anchor_points, stride_tensor = make_anchors(outputs, self.stride, offset=0.5)\n\n\n\n\n\n    #     #***********************************\n    #     # with debug_block(\"anchors\"):\n    #     #     tstats(\"anchor_points\", anchor_points)\n    #     #     tstats(\"stride_tensor\", stride_tensor)\n    #     #*****************************\n\n\n\n\n    #     idx = targets['idx'].reshape(-1, 1)\n    #     cls = targets['cls'].reshape(-1, 1)\n    #     box = targets['box']\n\n\n\n    #     #***************************\n    #     # # Sanity: class hợp lệ?\n    #     # assert (cls >= 0).all(), \"Found negative class id\"\n    #     # assert (cls < self.nc).all(), f\"Found class id >= nc ({self.nc})\"\n    #     #***************************\n\n\n\n\n\n\n    #     targets = torch.cat((idx, cls, box), dim=1).to(self.device)\n    #     if targets.shape[0] == 0:\n    #         gt = torch.zeros(batch_size, 0, 5, device=self.device)\n    #     else:\n    #         i = targets[:, 0]\n    #         _, counts = i.unique(return_counts=True)\n    #         counts = counts.to(dtype=torch.int32)\n    #         gt = torch.zeros(batch_size, counts.max(), 5, device=self.device)\n    #         for j in range(batch_size):\n    #             matches = i == j\n    #             n = matches.sum()\n    #             if n:\n    #                 gt[j, :n] = targets[matches, 1:]\n    #         x = gt[..., 1:5].mul_(input_size[[1, 0, 1, 0]])\n    #         y = torch.empty_like(x)\n    #         dw = x[..., 2] / 2  # half-width\n    #         dh = x[..., 3] / 2  # half-height\n    #         y[..., 0] = x[..., 0] - dw  # top left x\n    #         y[..., 1] = x[..., 1] - dh  # top left y\n    #         y[..., 2] = x[..., 0] + dw  # bottom right x\n    #         y[..., 3] = x[..., 1] + dh  # bottom right y\n    #         gt[..., 1:5] = y\n    #     gt_labels, gt_bboxes = gt.split((1, 4), 2)\n    #     mask_gt = gt_bboxes.sum(2, keepdim=True).gt_(0)\n\n    #     #***************************\n    #     # with debug_block(\"GT build\"):\n    #     #     tstats(\"gt_labels\", gt_labels)\n    #     #     tuniq(\"gt_labels uniq\", gt_labels.reshape(-1))\n    #     #     tstats(\"gt_bboxes\", gt_bboxes)\n    #     #     print(\"mask_gt sum:\", mask_gt.sum().item())\n    #     #***************************\n\n\n\n\n\n    #     pred_bboxes = self.box_decode(anchor_points, pred_distri)\n\n\n    #     #***************************\n    #     # with debug_block(\"decoded boxes\"):\n    #     #     tstats(\"pred_bboxes(decoded)\", pred_bboxes)\n    #     #***************************\n\n\n    #     assigned_targets = self.assigner(pred_scores.detach().sigmoid(),\n    #                                      (pred_bboxes.detach() * stride_tensor).type(gt_bboxes.dtype),\n    #                                      anchor_points * stride_tensor, gt_labels, gt_bboxes, mask_gt)\n    #     target_bboxes, target_scores, fg_mask = assigned_targets\n\n\n    #     #***************************\n    #     # with debug_block(\"after assigner\"):\n    #     #     tstats(\"target_bboxes\", target_bboxes)\n    #     #     tstats(\"target_scores\", target_scores)\n    #     #     print(\"target_scores > 0:\", (target_scores > 0).sum().item())\n    #     #     print(\"fg_mask sum:\", fg_mask.sum().item())\n    #     #***************************\n\n\n    #     # # SỬA: Quan trọng: dùng clamp để đảm bảo tensor cùng device, tránh Python int.\n    #     target_scores_sum = target_scores.sum().clamp(min=1.0)\n\n    #     loss_cls = self.cls_loss(pred_scores, target_scores.to(data_type)).sum() / target_scores_sum  # BCE\n\n    #     # #SỬA: scale lại cls loss theo số lượng foreground anchors nếu muốn\n    #     # fg_mask_cls = target_scores.sum(-1) > 0\n    #     # pred_scores_pos = pred_scores[fg_mask_cls]\n    #     # target_scores_pos = target_scores[fg_mask_cls]\n        \n    #     # if fg_mask_cls.sum() > 0:\n    #     #     loss_cls = self.cls_loss(pred_scores_pos, target_scores_pos).sum() / fg_mask_cls.sum()  # sum()/num_foreground\n    #     # else:\n    #     #     loss_cls = torch.zeros(1, device=pred_scores.device)\n\n    #     # SỐ foreground anchors (tính theo mask) #SỬA\n    #     #num_fg_anchors = fg_mask.sum().clamp(min=1.0)  \n        \n    #     # Classification loss (BCE) #SỬA\n    #     # Dùng mean trên lớp, sau đó sum trên foreground anchors\n    #     #loss_cls = (self.cls_loss(pred_scores, target_scores.to(pred_scores.dtype)).sum(dim=-1)  # sum over nc\n    #                 #[fg_mask]).sum() / num_fg_anchors\n\n    #     # with debug_block(\"cls loss\"):\n    #     #     print(\"target_scores_sum:\", float(target_scores_sum.detach().cpu()))\n    #     #     tstats(\"BCE elem\", self.cls_loss(pred_scores, target_scores.to(data_type)))\n    #     #     print(\"loss_cls:\", float(loss_cls.detach().cpu()))\n\n    #     # Box loss\n    #     loss_box = torch.zeros(1, device=self.device)\n    #     loss_dfl = torch.zeros(1, device=self.device)\n    #     if fg_mask.sum():\n    #         target_bboxes /= stride_tensor\n    #         loss_box, loss_dfl = self.box_loss(pred_distri,\n    #                                            pred_bboxes,\n    #                                            anchor_points,\n    #                                            target_bboxes,\n    #                                            target_scores,\n    #                                            target_scores_sum, fg_mask)\n    #     else:\n    #         print(\"NOTE: fg_mask.sum()==0 => loss_box=0, loss_dfl=0\")\n\n\n    #     # loss_box đã tính xong từ BoxLoss.forward\n    #     # print(\"loss_box (before gain):\", float(loss_box.detach().cpu()))\n    #     # print(\"box_gain:\", self.params['box'])\n        \n    #     loss_box *= self.params['box']  # box gain\n    #     loss_cls *= self.params['cls']  # cls gain\n    #     loss_dfl *= self.params['dfl']  # dfl gain\n\n    #     # with debug_block(\"final losses (after gain)\"):\n    #     #     print(f\"loss_box={float(loss_box.detach().cpu()):.6f} \"\n    #     #           f\"loss_cls={float(loss_cls.detach().cpu()):.6f} \"\n    #     #           f\"loss_dfl={float(loss_dfl.detach().cpu()):.6f}\")\n        \n    #     return loss_box, loss_cls, loss_dfl\n\n    def __call__(self, outputs, targets):\n        \"\"\"\n        Compute YOLOv8 loss (cls + box + DFL) for a batch of predictions and targets.\n        All tensors are ensured to be on the same device as outputs.\n        \"\"\"\n        # Lấy device của model/output\n        device = outputs[0].device\n        data_type = outputs[0].dtype\n        batch_size = outputs[0].shape[0]\n    \n        # reshape outputs\n        x = torch.cat([i.reshape(batch_size, self.no, -1) for i in outputs], dim=2)\n        pred_distri, pred_scores = x.split(split_size=(self.reg_max * 4, self.nc), dim=1)\n    \n        pred_scores = pred_scores.permute(0, 2, 1).contiguous()\n        pred_distri = pred_distri.permute(0, 2, 1).contiguous()\n    \n        # tạo anchor_points & stride_tensor\n        input_size = torch.tensor(outputs[0].shape[2:], device=device, dtype=data_type) * self.stride[0]\n        anchor_points, stride_tensor = make_anchors(outputs, self.stride, offset=0.5)\n        anchor_points = anchor_points.to(device)\n        stride_tensor = stride_tensor.to(device)\n    \n        # targets -> tensor trên cùng device\n        idx = targets['idx'].reshape(-1, 1)\n        cls = targets['cls'].reshape(-1, 1)\n        box = targets['box']\n        targets = torch.cat((idx, cls, box), dim=1).to(device)\n    \n        # build ground-truth tensor\n        if targets.shape[0] == 0:\n            gt = torch.zeros(batch_size, 0, 5, device=device)\n        else:\n            i = targets[:, 0]\n            _, counts = i.unique(return_counts=True)\n            counts = counts.to(dtype=torch.int32)\n            gt = torch.zeros(batch_size, counts.max(), 5, device=device)\n            for j in range(batch_size):\n                matches = i == j\n                n = matches.sum()\n                if n:\n                    gt[j, :n] = targets[matches, 1:]\n    \n            # convert center/wh -> x1y1x2y2\n            x = gt[..., 1:5].mul_(input_size[[1, 0, 1, 0]])\n            y = torch.empty_like(x)\n            dw = x[..., 2] / 2\n            dh = x[..., 3] / 2\n            y[..., 0] = x[..., 0] - dw\n            y[..., 1] = x[..., 1] - dh\n            y[..., 2] = x[..., 0] + dw\n            y[..., 3] = x[..., 1] + dh\n            gt[..., 1:5] = y\n    \n        gt_labels, gt_bboxes = gt.split((1, 4), 2)\n        mask_gt = gt_bboxes.sum(2, keepdim=True).gt_(0)\n    \n        # decode pred boxes\n        pred_bboxes = self.box_decode(anchor_points, pred_distri)\n    \n        # assign targets\n        assigned_targets = self.assigner(\n            pred_scores.detach().sigmoid(),\n            (pred_bboxes.detach() * stride_tensor).type(gt_bboxes.dtype),\n            anchor_points * stride_tensor,\n            gt_labels,\n            gt_bboxes,\n            mask_gt\n        )\n        target_bboxes, target_scores, fg_mask = assigned_targets\n    \n        # cls loss\n        target_scores_sum = target_scores.sum().clamp(min=1.0)\n        loss_cls = self.cls_loss(pred_scores, target_scores.to(pred_scores.dtype)).sum() / target_scores_sum\n    \n        # box + dfl loss\n        loss_box = torch.zeros(1, device=device)\n        loss_dfl = torch.zeros(1, device=device)\n        if fg_mask.sum():\n            target_bboxes = target_bboxes.to(device)\n            target_bboxes /= stride_tensor\n            loss_box, loss_dfl = self.box_loss(\n                pred_distri,\n                pred_bboxes,\n                anchor_points,\n                target_bboxes,\n                target_scores,\n                target_scores_sum,\n                fg_mask\n            )\n    \n        # scale losses\n        loss_box *= self.params['box']\n        loss_cls *= self.params['cls']\n        loss_dfl *= self.params['dfl']\n    \n        # trả về tất cả trên device của outputs\n        return loss_box.to(device), loss_cls.to(device), loss_dfl.to(device)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-23T07:55:11.555422Z","iopub.execute_input":"2025-08-23T07:55:11.555624Z","iopub.status.idle":"2025-08-23T07:55:11.634917Z","shell.execute_reply.started":"2025-08-23T07:55:11.555609Z","shell.execute_reply":"2025-08-23T07:55:11.634292Z"}},"outputs":[],"execution_count":54},{"id":"03efdf69","cell_type":"markdown","source":"# DATASET","metadata":{}},{"id":"8e6b8bed","cell_type":"code","source":"import math\nimport os\nimport random\n\nimport cv2\nimport numpy\nimport torch\nfrom PIL import Image\nfrom torch.utils import data\n\nFORMATS = 'bmp', 'dng', 'jpeg', 'jpg', 'mpo', 'png', 'tif', 'tiff', 'webp'\n\nclass Dataset(data.Dataset):\n    def __init__(self, filenames, input_size, params, augment):\n        self.params = params\n        self.mosaic = augment\n        self.augment = augment\n        self.input_size = input_size\n\n        # Read labels\n        labels = self.load_label(filenames)\n        self.labels = list(labels.values())\n        self.filenames = list(labels.keys())  # update\n        self.n = len(self.filenames)  # number of samples\n        self.indices = range(self.n)\n        # Albumentations (optional, only used if package is installed)\n        self.albumentations = Albumentations()\n\n    def __getitem__(self, index):\n        index = self.indices[index]\n\n        params = self.params\n        mosaic = self.mosaic and random.random() < params['mosaic']\n\n        if mosaic:\n            # Load MOSAIC\n            image, label = self.load_mosaic(index, params)\n            # MixUp augmentation\n            if random.random() < params['mix_up']:\n                index = random.choice(self.indices)\n                mix_image1, mix_label1 = image, label\n                mix_image2, mix_label2 = self.load_mosaic(index, params)\n\n                image, label = mix_up(mix_image1, mix_label1, mix_image2, mix_label2)\n        else:\n            # Load image\n            image, shape = self.load_image(index)\n            h, w = image.shape[:2]\n\n            # Resize\n            image, ratio, pad = resize(image, self.input_size, self.augment)\n\n            label = self.labels[index].copy()\n            if label.size:\n                label[:, 1:] = wh2xy(label[:, 1:], ratio[0] * w, ratio[1] * h, pad[0], pad[1])\n            if self.augment:\n                image, label = random_perspective(image, label, params)\n\n        nl = len(label)  # number of labels\n        h, w = image.shape[:2]\n        cls = label[:, 0:1]\n        box = label[:, 1:5]\n        box = xy2wh(box, w, h)\n\n        if self.augment:\n            # Albumentations\n            image, box, cls = self.albumentations(image, box, cls)\n            nl = len(box)  # update after albumentations\n            # HSV color-space\n            augment_hsv(image, params)\n            # Flip up-down\n            if random.random() < params['flip_ud']:\n                image = numpy.flipud(image)\n                if nl:\n                    box[:, 1] = 1 - box[:, 1]\n            # Flip left-right\n            if random.random() < params['flip_lr']:\n                image = numpy.fliplr(image)\n                if nl:\n                    box[:, 0] = 1 - box[:, 0]\n\n        target_cls = torch.zeros((nl, 1))\n        target_box = torch.zeros((nl, 4))\n        if nl:\n            target_cls = torch.from_numpy(cls)\n            target_box = torch.from_numpy(box)\n\n        # Convert HWC to CHW, BGR to RGB\n        sample = image.transpose((2, 0, 1))[::-1]\n        sample = numpy.ascontiguousarray(sample)\n\n        return torch.from_numpy(sample), target_cls, target_box, torch.zeros(nl)\n\n    def __len__(self):\n        return len(self.filenames)\n\n    def load_image(self, i):\n        image = cv2.imread(self.filenames[i])\n        h, w = image.shape[:2]\n        r = self.input_size / max(h, w)\n        if r != 1:\n            image = cv2.resize(image,\n                               dsize=(int(w * r), int(h * r)),\n                               interpolation=resample() if self.augment else cv2.INTER_LINEAR)\n        return image, (h, w)\n\n    def load_mosaic(self, index, params):\n        label4 = []\n        border = [-self.input_size // 2, -self.input_size // 2]\n        image4 = numpy.full((self.input_size * 2, self.input_size * 2, 3), 0, dtype=numpy.uint8)\n        y1a, y2a, x1a, x2a, y1b, y2b, x1b, x2b = (None, None, None, None, None, None, None, None)\n\n        xc = int(random.uniform(-border[0], 2 * self.input_size + border[1]))\n        yc = int(random.uniform(-border[0], 2 * self.input_size + border[1]))\n\n        indices = [index] + random.choices(self.indices, k=3)\n        random.shuffle(indices)\n\n        for i, index in enumerate(indices):\n            # Load image\n            image, _ = self.load_image(index)\n            shape = image.shape\n            if i == 0:  # top left\n                x1a = max(xc - shape[1], 0)\n                y1a = max(yc - shape[0], 0)\n                x2a = xc\n                y2a = yc\n                x1b = shape[1] - (x2a - x1a)\n                y1b = shape[0] - (y2a - y1a)\n                x2b = shape[1]\n                y2b = shape[0]\n            if i == 1:  # top right\n                x1a = xc\n                y1a = max(yc - shape[0], 0)\n                x2a = min(xc + shape[1], self.input_size * 2)\n                y2a = yc\n                x1b = 0\n                y1b = shape[0] - (y2a - y1a)\n                x2b = min(shape[1], x2a - x1a)\n                y2b = shape[0]\n            if i == 2:  # bottom left\n                x1a = max(xc - shape[1], 0)\n                y1a = yc\n                x2a = xc\n                y2a = min(self.input_size * 2, yc + shape[0])\n                x1b = shape[1] - (x2a - x1a)\n                y1b = 0\n                x2b = shape[1]\n                y2b = min(y2a - y1a, shape[0])\n            if i == 3:  # bottom right\n                x1a = xc\n                y1a = yc\n                x2a = min(xc + shape[1], self.input_size * 2)\n                y2a = min(self.input_size * 2, yc + shape[0])\n                x1b = 0\n                y1b = 0\n                x2b = min(shape[1], x2a - x1a)\n                y2b = min(y2a - y1a, shape[0])\n\n            pad_w = x1a - x1b\n            pad_h = y1a - y1b\n            image4[y1a:y2a, x1a:x2a] = image[y1b:y2b, x1b:x2b]\n\n            # Labels\n            label = self.labels[index].copy()\n            if len(label):\n                label[:, 1:] = wh2xy(label[:, 1:], shape[1], shape[0], pad_w, pad_h)\n            label4.append(label)\n\n        # Concat/clip labels\n        label4 = numpy.concatenate(label4, 0)\n        for x in label4[:, 1:]:\n            numpy.clip(x, 0, 2 * self.input_size, out=x)\n\n        # Augment\n        image4, label4 = random_perspective(image4, label4, params, border)\n\n        return image4, label4\n\n    @staticmethod\n    def collate_fn(batch):\n        samples, cls, box, indices = zip(*batch)\n\n        cls = torch.cat(cls, dim=0)\n        box = torch.cat(box, dim=0)\n\n        new_indices = list(indices)\n        for i in range(len(indices)):\n            new_indices[i] += i\n        indices = torch.cat(new_indices, dim=0)\n\n        targets = {'cls': cls,\n                   'box': box,\n                   'idx': indices}\n        return torch.stack(samples, dim=0), targets\n\n    @staticmethod\n    def load_label(filenames):\n        x = {}\n        for filename in filenames:\n            try:\n                # verify images\n                with open(filename, 'rb') as f:\n                    image = Image.open(f)\n                    image.verify()  # PIL verify\n                shape = image.size  # image size\n                assert (shape[0] > 9) & (shape[1] > 9), f'image size {shape} <10 pixels'\n                assert image.format.lower() in FORMATS, f'invalid image format {image.format}'\n\n                # verify labels\n                a = f'{os.sep}images{os.sep}'\n                b = f'{os.sep}labels{os.sep}'\n                label_path = b.join(filename.rsplit(a, 1)).rsplit('.', 1)[0] + '.txt'\n                if os.path.isfile(b.join(filename.rsplit(a, 1)).rsplit('.', 1)[0] + '.txt'):\n                    with open(label_path) as f:\n                        label = [x.split() for x in f.read().strip().splitlines() if len(x)]\n                        label = numpy.array(label, dtype=numpy.float32)\n                    nl = len(label)\n                    if nl:\n                        assert (label >= 0).all()\n                        assert label.shape[1] == 5\n                        assert (label[:, 1:] <= 1).all()\n                        _, i = numpy.unique(label, axis=0, return_index=True)\n                        if len(i) < nl:  # duplicate row check\n                            label = label[i]  # remove duplicates\n                    else:\n                        label = numpy.zeros((0, 5), dtype=numpy.float32)\n                else:\n                    label = numpy.zeros((0, 5), dtype=numpy.float32)\n            except FileNotFoundError:\n                label = numpy.zeros((0, 5), dtype=numpy.float32)\n            except AssertionError:\n                continue\n            x[filename] = label\n        return x\n\n\ndef wh2xy(x, w, h, pad_w=0, pad_h=0):\n    # Convert nx4 boxes\n    # from [x, y, w, h] normalized to [x1, y1, x2, y2] where xy1=top-left, xy2=bottom-right\n    y = numpy.copy(x)\n    y[:, 0] = w * (x[:, 0] - x[:, 2] / 2) + pad_w  # top left x\n    y[:, 1] = h * (x[:, 1] - x[:, 3] / 2) + pad_h  # top left y\n    y[:, 2] = w * (x[:, 0] + x[:, 2] / 2) + pad_w  # bottom right x\n    y[:, 3] = h * (x[:, 1] + x[:, 3] / 2) + pad_h  # bottom right y\n    return y\n\n\ndef xy2wh(x, w, h):\n    # warning: inplace clip\n    x[:, [0, 2]] = x[:, [0, 2]].clip(0, w - 1E-3)  # x1, x2\n    x[:, [1, 3]] = x[:, [1, 3]].clip(0, h - 1E-3)  # y1, y2\n\n    # Convert nx4 boxes\n    # from [x1, y1, x2, y2] to [x, y, w, h] normalized where xy1=top-left, xy2=bottom-right\n    y = numpy.copy(x)\n    y[:, 0] = ((x[:, 0] + x[:, 2]) / 2) / w  # x center\n    y[:, 1] = ((x[:, 1] + x[:, 3]) / 2) / h  # y center\n    y[:, 2] = (x[:, 2] - x[:, 0]) / w  # width\n    y[:, 3] = (x[:, 3] - x[:, 1]) / h  # height\n    return y\n\n\ndef resample():\n    choices = (cv2.INTER_AREA,\n               cv2.INTER_CUBIC,\n               cv2.INTER_LINEAR,\n               cv2.INTER_NEAREST,\n               cv2.INTER_LANCZOS4)\n    return random.choice(seq=choices)\n\n\ndef augment_hsv(image, params):\n    # HSV color-space augmentation\n    h = params['hsv_h']\n    s = params['hsv_s']\n    v = params['hsv_v']\n\n    r = numpy.random.uniform(-1, 1, 3) * [h, s, v] + 1\n    h, s, v = cv2.split(cv2.cvtColor(image, cv2.COLOR_BGR2HSV))\n\n    x = numpy.arange(0, 256, dtype=r.dtype)\n    lut_h = ((x * r[0]) % 180).astype('uint8')\n    lut_s = numpy.clip(x * r[1], 0, 255).astype('uint8')\n    lut_v = numpy.clip(x * r[2], 0, 255).astype('uint8')\n\n    hsv = cv2.merge((cv2.LUT(h, lut_h), cv2.LUT(s, lut_s), cv2.LUT(v, lut_v)))\n    cv2.cvtColor(hsv, cv2.COLOR_HSV2BGR, dst=image)  # no return needed\n\n\ndef resize(image, input_size, augment):\n    # Resize and pad image while meeting stride-multiple constraints\n    shape = image.shape[:2]  # current shape [height, width]\n\n    # Scale ratio (new / old)\n    r = min(input_size / shape[0], input_size / shape[1])\n    if not augment:  # only scale down, do not scale up (for better val mAP)\n        r = min(r, 1.0)\n\n    # Compute padding\n    pad = int(round(shape[1] * r)), int(round(shape[0] * r))\n    w = (input_size - pad[0]) / 2\n    h = (input_size - pad[1]) / 2\n\n    if shape[::-1] != pad:  # resize\n        image = cv2.resize(image,\n                           dsize=pad,\n                           interpolation=resample() if augment else cv2.INTER_LINEAR)\n    top, bottom = int(round(h - 0.1)), int(round(h + 0.1))\n    left, right = int(round(w - 0.1)), int(round(w + 0.1))\n    image = cv2.copyMakeBorder(image, top, bottom, left, right, cv2.BORDER_CONSTANT)  # add border\n    return image, (r, r), (w, h)\n\n\ndef candidates(box1, box2):\n    # box1(4,n), box2(4,n)\n    w1, h1 = box1[2] - box1[0], box1[3] - box1[1]\n    w2, h2 = box2[2] - box2[0], box2[3] - box2[1]\n    aspect_ratio = numpy.maximum(w2 / (h2 + 1e-16), h2 / (w2 + 1e-16))  # aspect ratio\n    return (w2 > 2) & (h2 > 2) & (w2 * h2 / (w1 * h1 + 1e-16) > 0.1) & (aspect_ratio < 100)\n\n\ndef random_perspective(image, label, params, border=(0, 0)):\n    h = image.shape[0] + border[0] * 2\n    w = image.shape[1] + border[1] * 2\n\n    # Center\n    center = numpy.eye(3)\n    center[0, 2] = -image.shape[1] / 2  # x translation (pixels)\n    center[1, 2] = -image.shape[0] / 2  # y translation (pixels)\n\n    # Perspective\n    perspective = numpy.eye(3)\n\n    # Rotation and Scale\n    rotate = numpy.eye(3)\n    a = random.uniform(-params['degrees'], params['degrees'])\n    s = random.uniform(1 - params['scale'], 1 + params['scale'])\n    rotate[:2] = cv2.getRotationMatrix2D(angle=a, center=(0, 0), scale=s)\n\n    # Shear\n    shear = numpy.eye(3)\n    shear[0, 1] = math.tan(random.uniform(-params['shear'], params['shear']) * math.pi / 180)\n    shear[1, 0] = math.tan(random.uniform(-params['shear'], params['shear']) * math.pi / 180)\n\n    # Translation\n    translate = numpy.eye(3)\n    translate[0, 2] = random.uniform(0.5 - params['translate'], 0.5 + params['translate']) * w\n    translate[1, 2] = random.uniform(0.5 - params['translate'], 0.5 + params['translate']) * h\n\n    # Combined rotation matrix, order of operations (right to left) is IMPORTANT\n    matrix = translate @ shear @ rotate @ perspective @ center\n    if (border[0] != 0) or (border[1] != 0) or (matrix != numpy.eye(3)).any():  # image changed\n        image = cv2.warpAffine(image, matrix[:2], dsize=(w, h), borderValue=(0, 0, 0))\n\n    # Transform label coordinates\n    n = len(label)\n    if n:\n        xy = numpy.ones((n * 4, 3))\n        xy[:, :2] = label[:, [1, 2, 3, 4, 1, 4, 3, 2]].reshape(n * 4, 2)  # x1y1, x2y2, x1y2, x2y1\n        xy = xy @ matrix.T  # transform\n        xy = xy[:, :2].reshape(n, 8)  # perspective rescale or affine\n\n        # create new boxes\n        x = xy[:, [0, 2, 4, 6]]\n        y = xy[:, [1, 3, 5, 7]]\n        box = numpy.concatenate((x.min(1), y.min(1), x.max(1), y.max(1))).reshape(4, n).T\n\n        # clip\n        box[:, [0, 2]] = box[:, [0, 2]].clip(0, w)\n        box[:, [1, 3]] = box[:, [1, 3]].clip(0, h)\n        # filter candidates\n        indices = candidates(box1=label[:, 1:5].T * s, box2=box.T)\n\n        label = label[indices]\n        label[:, 1:5] = box[indices]\n\n    return image, label\n\n\ndef mix_up(image1, label1, image2, label2):\n    # Applies MixUp augmentation https://arxiv.org/pdf/1710.09412.pdf\n    alpha = numpy.random.beta(a=32.0, b=32.0)  # mix-up ratio, alpha=beta=32.0\n    image = (image1 * alpha + image2 * (1 - alpha)).astype(numpy.uint8)\n    label = numpy.concatenate((label1, label2), 0)\n    return image, label\n\n\nclass Albumentations:\n    def __init__(self):\n        self.transform = None\n        try:\n            import albumentations\n\n            transforms = [albumentations.Blur(p=0.01),\n                          albumentations.CLAHE(p=0.01),\n                          albumentations.ToGray(p=0.01),\n                          albumentations.MedianBlur(p=0.01)]\n            self.transform = albumentations.Compose(transforms,\n                                                    albumentations.BboxParams('yolo', ['class_labels']))\n\n        except ImportError:  # package not installed, skip\n            pass\n\n    def __call__(self, image, box, cls):\n        if self.transform:\n            x = self.transform(image=image,\n                               bboxes=box,\n                               class_labels=cls)\n            image = x['image']\n            box = numpy.array(x['bboxes'])\n            cls = numpy.array(x['class_labels'])\n        return image, box, cls","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-23T07:55:11.635709Z","iopub.execute_input":"2025-08-23T07:55:11.635901Z","iopub.status.idle":"2025-08-23T07:55:11.678788Z","shell.execute_reply.started":"2025-08-23T07:55:11.635885Z","shell.execute_reply":"2025-08-23T07:55:11.678187Z"}},"outputs":[],"execution_count":55},{"id":"c5f1a711","cell_type":"code","source":"# PARAMS\nparams = {\n    'min_lr': 0.0001,\n    'max_lr': 0.01,\n    'momentum': 0.937,\n    'weight_decay': 0.0005,\n    'warmup_epochs': 3.0,\n    'box': 7.5,\n    'cls': 0.5,\n    'dfl': 1.5,\n\n    # --- Tắt augmentation ---\n    'hsv_h': 0.0,\n    'hsv_s': 0.0,\n    'hsv_v': 0.0,\n    'degrees': 0.0,\n    'translate': 0.0,\n    'scale': 1.0,\n    'shear': 0.0,\n    'flip_ud': 0.0,\n    'flip_lr': 0.0,\n    'mosaic': 0.0,\n    'mix_up': 0.0,\n\n    # --- Dataset ---\n    'nc': 5,\n    'names': ['Elephant', 'Giraffe', 'Leopard', 'Lion', 'Zebra']\n}","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-23T07:55:11.679509Z","iopub.execute_input":"2025-08-23T07:55:11.679738Z","iopub.status.idle":"2025-08-23T07:55:11.696020Z","shell.execute_reply.started":"2025-08-23T07:55:11.679715Z","shell.execute_reply":"2025-08-23T07:55:11.695405Z"}},"outputs":[],"execution_count":56},{"id":"c8605e0b","cell_type":"code","source":"train_dir = os.path.join(DATASET_PATH, \"train\", \"images\")\n\nfilenames_train = []\nfor filename in os.listdir(train_dir):\n    if filename.endswith(('.jpg', '.png', '.jpeg')):\n        filenames_train.append(os.path.join(train_dir, filename))\n\ninput_size = 640\n\n# Tạo Dataset cho tập train\ntrain_data = Dataset(\n    filenames_train,\n    input_size,\n    params,   # đã được định nghĩa ở cell trước\n    augment=False   # False = không dùng augmentation\n)\n\n# DataLoader\ntrain_loader = DataLoader(\n    train_data,\n    batch_size=32,\n    num_workers=2,\n    pin_memory=True,\n    collate_fn=Dataset.collate_fn\n)\n\nval_dir = os.path.join(DATASET_PATH, \"valid\", \"images\")\n\nfilenames_val = [os.path.join(val_dir, f) \n                 for f in os.listdir(val_dir) \n                 if f.endswith(('.jpg', '.png', '.jpeg'))]\n\nval_dataset = Dataset(\n    filenames_val,\n    input_size,\n    params,\n    augment=False   # thường không augment validation\n)\n\nval_loader = DataLoader(\n    val_dataset,\n    batch_size=32,      \n    num_workers=2,\n    pin_memory=True,\n    collate_fn=Dataset.collate_fn\n)\n\n\n\n\nprint(f\"Train_loader : {len(train_loader)} batches\")\n\nprint(f\"Val_loader: {len(val_loader)} batches\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-23T08:25:27.818350Z","iopub.execute_input":"2025-08-23T08:25:27.818656Z","iopub.status.idle":"2025-08-23T08:25:50.396350Z","shell.execute_reply.started":"2025-08-23T08:25:27.818636Z","shell.execute_reply":"2025-08-23T08:25:50.395364Z"}},"outputs":[{"name":"stdout","text":"Train_loader : 281 batches\nVal_loader: 81 batches\n","output_type":"stream"}],"execution_count":76},{"id":"b70112da","cell_type":"code","source":"batch=next(iter(train_loader))\nprint(\"All keys in batch      : \", batch[1].keys())\nprint(f\"Input batch shape      : \", batch[0].shape)\nprint(f\"Classification scores  : {batch[1]['cls'].shape}\")\nprint(f\"Box coordinates        : {batch[1]['box'].shape}\")\nprint(f\"Index identifier (which score belongs to which image): {batch[1]['idx'].shape}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-23T07:55:32.909586Z","iopub.execute_input":"2025-08-23T07:55:32.909832Z","iopub.status.idle":"2025-08-23T07:55:33.459825Z","shell.execute_reply.started":"2025-08-23T07:55:32.909808Z","shell.execute_reply":"2025-08-23T07:55:33.458985Z"}},"outputs":[{"name":"stdout","text":"All keys in batch      :  dict_keys(['cls', 'box', 'idx'])\nInput batch shape      :  torch.Size([32, 3, 640, 640])\nClassification scores  : torch.Size([45, 1])\nBox coordinates        : torch.Size([45, 4])\nIndex identifier (which score belongs to which image): torch.Size([45])\n","output_type":"stream"}],"execution_count":58},{"id":"d2dfd659-e0e7-4806-9665-b5f67f613d09","cell_type":"markdown","source":"# GPU FULL","metadata":{}},{"id":"be56cb37-2e41-4645-b323-eaa99b1e45d0","cell_type":"code","source":"import torch\n\ndef bbox_iou_torch(box1, box2):\n    \"\"\"\n    Compute IoU between two sets of boxes using PyTorch (GPU compatible)\n    box1: (N,4) xyxy\n    box2: (M,4) xyxy\n    return: (N,M) IoU matrix\n    \"\"\"\n    N = box1.shape[0]\n    M = box2.shape[0]\n\n    inter_x1 = torch.max(box1[:, None, 0], box2[None, :, 0])\n    inter_y1 = torch.max(box1[:, None, 1], box2[None, :, 1])\n    inter_x2 = torch.min(box1[:, None, 2], box2[None, :, 2])\n    inter_y2 = torch.min(box1[:, None, 3], box2[None, :, 3])\n\n    inter_w = torch.clamp(inter_x2 - inter_x1, min=0)\n    inter_h = torch.clamp(inter_y2 - inter_y1, min=0)\n    inter_area = inter_w * inter_h\n\n    area1 = (box1[:, 2] - box1[:, 0]) * (box1[:, 3] - box1[:, 1])\n    area2 = (box2[:, 2] - box2[:, 0]) * (box2[:, 3] - box2[:, 1])\n    union = area1[:, None] + area2[None, :] - inter_area\n\n    iou = inter_area / (union + 1e-16)\n    return iou\n\n\ndef match_detections_torch(pred_bboxes, pred_conf, pred_classes, gt_bboxes, gt_classes, iou_threshold=0.5):\n    \"\"\"\n    GPU version of TP matching\n    \"\"\"\n    device = pred_bboxes.device\n    num_pred = pred_bboxes.shape[0]\n    tp = torch.zeros((num_pred,), device=device)\n\n    if gt_bboxes.shape[0] == 0 or num_pred == 0:\n        return tp\n\n    sort_idx = torch.argsort(-pred_conf)\n    pred_bboxes = pred_bboxes[sort_idx]\n    pred_classes = pred_classes[sort_idx]\n\n    assigned_gt = torch.zeros(gt_bboxes.shape[0], dtype=torch.bool, device=device)\n    ious = bbox_iou_torch(pred_bboxes, gt_bboxes)\n\n    for i in range(num_pred):\n        cls_matches = (pred_classes[i] == gt_classes)\n        iou_matches = ious[i] >= iou_threshold\n        matches = cls_matches & iou_matches & (~assigned_gt)\n        if matches.any():\n            gt_idx = torch.argmax(ious[i] * matches.float())\n            tp[sort_idx[i]] = 1\n            assigned_gt[gt_idx] = True\n\n    return tp\n\n\ndef build_tp_matrix_torch(pred_bboxes, pred_conf, pred_classes, gt_bboxes, gt_classes):\n    \"\"\"\n    Trả về ma trận TP (num_preds, 10) trên GPU\n    \"\"\"\n    device = pred_bboxes.device\n    iou_thresholds = torch.arange(0.5, 1.0, 0.05, device=device)\n    num_preds = pred_bboxes.shape[0]\n    tp_matrix = torch.zeros((num_preds, len(iou_thresholds)), device=device)\n\n    for j, thr in enumerate(iou_thresholds):\n        tp_matrix[:, j] = match_detections_torch(\n            pred_bboxes, pred_conf, pred_classes, gt_bboxes, gt_classes, iou_threshold=thr\n        )\n\n    return tp_matrix\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-23T08:57:49.967563Z","iopub.execute_input":"2025-08-23T08:57:49.968350Z","iopub.status.idle":"2025-08-23T08:57:49.978258Z","shell.execute_reply.started":"2025-08-23T08:57:49.968323Z","shell.execute_reply":"2025-08-23T08:57:49.977493Z"}},"outputs":[],"execution_count":89},{"id":"d64369b9-90ff-4fc4-8e70-e20d38c17909","cell_type":"code","source":"import torch\nimport numpy as np\n\ndef compute_ap_torch(tp, conf, pred_cls, target_cls, eps=1e-16):\n    \"\"\"\n    Compute average precision on GPU tensors.\n    tp: torch.Tensor (n_preds, n_iou_thresholds)\n    conf: torch.Tensor (n_preds,)\n    pred_cls: torch.Tensor (n_preds,)\n    target_cls: torch.Tensor (n_targets,)\n    Returns: tp, fp, m_pre, m_rec, map50, mean_ap (all as float scalars)\n    \"\"\"\n    device = tp.device\n\n    # Sort by confidence\n    conf_sort, sort_idx = torch.sort(conf, descending=True)\n    tp = tp[sort_idx]\n    pred_cls = pred_cls[sort_idx]\n\n    unique_classes, nt = torch.unique(target_cls, return_counts=True)\n    nc = unique_classes.shape[0]\n    n_iou = tp.shape[1]\n\n    ap = torch.zeros((nc, n_iou), device=device)\n    p = torch.zeros((nc, 1000), device=device)\n    r = torch.zeros((nc, 1000), device=device)\n    px = torch.linspace(0, 1, 1000, device=device)\n\n    for ci, c in enumerate(unique_classes):\n        mask = pred_cls == c\n        nl = nt[ci].item()\n        no = mask.sum().item()\n        if no == 0 or nl == 0:\n            continue\n\n        tpc = tp[mask].cumsum(dim=0)\n        fpc = (1 - tp[mask]).cumsum(dim=0)\n        recall = tpc / (nl + eps)\n        precision = tpc / (tpc + fpc)\n\n        # Interpolation for plotting (optional)\n        r[ci] = torch.interp(-px, -conf_sort[mask], recall[:, 0], left=0.0)\n        p[ci] = torch.interp(-px, -conf_sort[mask], precision[:, 0], left=1.0)\n\n        # Compute AP for each IoU threshold\n        for j in range(n_iou):\n            m_rec = torch.cat([torch.tensor([0.0], device=device), recall[:, j], torch.tensor([1.0], device=device)])\n            m_pre = torch.cat([torch.tensor([1.0], device=device), precision[:, j], torch.tensor([0.0], device=device)])\n            m_pre = torch.flip(torch.maximum.accumulate(torch.flip(m_pre, dims=[0])), dims=[0])\n            x = torch.linspace(0, 1, 101, device=device)\n            ap[ci, j] = torch.trapz(torch.interp(x, m_rec, m_pre), x)\n\n    # F1 score\n    f1 = 2 * p * r / (p + r + eps)\n    i = torch.argmax(f1.mean(0))\n    p_mean, r_mean, f1_mean = p[:, i], r[:, i], f1[:, i]\n    tp_total = (r_mean * nt.to(device)).round()\n    fp_total = (tp_total / (p_mean + eps) - tp_total).round()\n    ap50, ap_mean = ap[:, 0], ap.mean(1)\n\n    map50, mean_ap = ap50.mean().item(), ap_mean.mean().item()\n    return tp_total.cpu().numpy(), fp_total.cpu().numpy(), p_mean.mean().item(), r_mean.mean().item(), map50, mean_ap\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-23T08:57:52.294315Z","iopub.execute_input":"2025-08-23T08:57:52.295299Z","iopub.status.idle":"2025-08-23T08:57:52.305762Z","shell.execute_reply.started":"2025-08-23T08:57:52.295266Z","shell.execute_reply":"2025-08-23T08:57:52.304945Z"}},"outputs":[],"execution_count":90},{"id":"46fbe672-42af-4d26-b156-458763cddfa1","cell_type":"code","source":"import torch\nfrom torch.utils.data import DataLoader\nfrom tqdm import tqdm\n\nNUM_CLASSES = 5\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'\ntorch.manual_seed(1337)\n\n# ===================== Model & Optimizer =====================\nmodel = MyYolo(version='n').to(device)\nprint(f\"{sum(p.numel() for p in model.parameters())/1e6:.2f} million parameters\")\nprint(f\"Number of classes (nc): {model.nc}\")\n\ncriterion = ComputeLoss(model, params)\noptimizer = torch.optim.AdamW(model.parameters(), lr=0.01)\nnum_epochs = 10\n\n# ===================== Training + Validation =====================\nfor epoch in range(num_epochs):\n    # -------- Training --------\n    model.train()\n    epoch_loss = 0.0\n    train_pbar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs} - Train\")\n\n    for imgs, targets in train_pbar:\n        imgs = imgs.to(device, dtype=torch.float32)\n        targets = {k: v.to(device) if isinstance(v, torch.Tensor) else v for k, v in targets.items()}\n\n        outputs = model(imgs)\n        box_loss, cls_loss, dfl_loss = criterion(outputs, targets)\n        loss = box_loss + cls_loss + dfl_loss\n\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n        epoch_loss += loss.item()\n\n        train_pbar.set_postfix({\n            \"Total\": f\"{loss.item():.4f}\",\n            \"Cls\": f\"{cls_loss.item():.4f}\",\n            \"Box\": f\"{box_loss.item():.4f}\",\n            \"DFL\": f\"{dfl_loss.item():.4f}\"\n        })\n\n    print(f\"Epoch {epoch+1}/{num_epochs} | Avg Loss: {epoch_loss/len(train_loader):.4f}\")\n\n    # -------- Validation --------\n    all_tp, all_conf, all_pred_cls, all_target_cls = [], [], [], []\n\n    with torch.no_grad():\n        # val_pbar = tqdm(val_loader, desc=f\"Epoch {epoch+1}/{num_epochs} - Val\")\n        val_pbar = tqdm(val_loader, total=len(val_loader), desc=f\"Epoch {epoch+1}/{num_epochs} - Val\")\n\n        for images, targets in val_pbar:\n            images = images.to(device, dtype=torch.float32)\n            targets = {k: v.to(device) if isinstance(v, torch.Tensor) else v for k, v in targets.items()}\n\n            outputs = model(images)\n            B = images.shape[0]\n            x = torch.cat([i.view(B, model.head.no, -1) for i in outputs], dim=2)\n            pred_distri, pred_scores = x.split(split_size=(16*4, NUM_CLASSES), dim=1)\n            pred_scores = pred_scores.permute(0, 2, 1).contiguous()\n            pred_distri = pred_distri.permute(0, 2, 1).contiguous()\n\n            # Anchors\n            anchor_points, stride_tensor = make_anchors(outputs, model.head.stride, offset=0.5)\n            anchor_points = anchor_points.to(device)\n            pred_bboxes = criterion.box_decode(anchor_points, pred_distri)\n            pred_bboxes = pred_bboxes.view(-1, 4)\n\n            # Predicted class & confidence\n            pred_scores_flat = pred_scores.view(-1, NUM_CLASSES)\n            pred_classes = pred_scores_flat.argmax(1)\n            pred_conf = pred_scores_flat.max(1).values\n\n            # Match detections (GPU-native)\n            tp_matrix = build_tp_matrix_torch(pred_bboxes, pred_conf, pred_classes, targets['box'], targets['cls'])\n\n            # Append tensors (still on GPU)\n            all_tp.append(tp_matrix)\n            all_conf.append(pred_conf)\n            all_pred_cls.append(pred_classes)\n            all_target_cls.append(targets['cls'])\n\n        # Concatenate all batches (GPU)\n        all_tp = torch.cat(all_tp, dim=0)\n        all_conf = torch.cat(all_conf, dim=0)\n        all_pred_cls = torch.cat(all_pred_cls, dim=0)\n        all_target_cls = torch.cat(all_target_cls, dim=0)\n\n        # Compute mAP (GPU)\n        tp_arr, fp_arr, m_pre, m_rec, map50, mean_ap = compute_ap_torch(\n            all_tp, all_conf, all_pred_cls, all_target_cls\n        )\n\n        # Hiển thị kết quả ngay trên tqdm\n        val_pbar.set_postfix({\n            \"mAP50\": f\"{map50:.4f}\",\n            \"mAP50-95\": f\"{mean_ap:.4f}\"\n        })\n\n    print(f\"Validation metrics: mAP50: {map50:.4f}, mAP50-95: {mean_ap:.4f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-23T09:08:28.479586Z","iopub.execute_input":"2025-08-23T09:08:28.480334Z","iopub.status.idle":"2025-08-23T09:11:46.551203Z","shell.execute_reply.started":"2025-08-23T09:08:28.480302Z","shell.execute_reply":"2025-08-23T09:11:46.550122Z"},"collapsed":true,"jupyter":{"outputs_hidden":true}},"outputs":[{"name":"stdout","text":"2.66 million parameters\nNumber of classes (nc): 5\n","output_type":"stream"},{"name":"stderr","text":"Epoch 1/10 - Train: 100%|██████████| 281/281 [01:46<00:00,  2.63it/s, Total=13.1772, Cls=7.8614, Box=2.1138, DFL=3.2020]    \n","output_type":"stream"},{"name":"stdout","text":"Epoch 1/10 | Avg Loss: 232.9491\n","output_type":"stream"},{"name":"stderr","text":"Epoch 1/10 - Val:   0%|          | 0/81 [01:31<?, ?it/s]\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_36/3769370569.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m             \u001b[0;31m# Match detections (GPU-native)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 77\u001b[0;31m             \u001b[0mtp_matrix\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuild_tp_matrix_torch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred_bboxes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpred_conf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpred_classes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'box'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'cls'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     78\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m             \u001b[0;31m# Append tensors (still on GPU)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipykernel_36/1553783507.py\u001b[0m in \u001b[0;36mbuild_tp_matrix_torch\u001b[0;34m(pred_bboxes, pred_conf, pred_classes, gt_bboxes, gt_classes)\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mthr\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miou_thresholds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m         tp_matrix[:, j] = match_detections_torch(\n\u001b[0m\u001b[1;32m     71\u001b[0m             \u001b[0mpred_bboxes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpred_conf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpred_classes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgt_bboxes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgt_classes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miou_threshold\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mthr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m         )\n","\u001b[0;32m/tmp/ipykernel_36/1553783507.py\u001b[0m in \u001b[0;36mmatch_detections_torch\u001b[0;34m(pred_bboxes, pred_conf, pred_classes, gt_bboxes, gt_classes, iou_threshold)\u001b[0m\n\u001b[1;32m     50\u001b[0m         \u001b[0miou_matches\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mious\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0miou_threshold\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m         \u001b[0mmatches\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcls_matches\u001b[0m \u001b[0;34m&\u001b[0m \u001b[0miou_matches\u001b[0m \u001b[0;34m&\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m~\u001b[0m\u001b[0massigned_gt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0mmatches\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0many\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m             \u001b[0mgt_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mious\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mmatches\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m             \u001b[0mtp\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msort_idx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}],"execution_count":93},{"id":"68b528ce-0629-4e4d-a0d3-3deed219731e","cell_type":"code","source":"for epoch in range(num_epochs):\n    # -------- Training --------\n    model.train()\n\n    # -------- Validation --------\n    all_tp, all_conf, all_pred_cls, all_target_cls = [], [], [], []\n    \n    with torch.no_grad():\n        val_pbar = tqdm(val_loader, total=len(val_loader), desc=f\"Epoch {epoch+1}/{num_epochs} - Val\")\n        for images, targets in val_pbar:\n            images = images.to(device, dtype=torch.float32)\n            targets = {k: v.to(device) if isinstance(v, torch.Tensor) else v for k, v in targets.items()}\n    \n            outputs = model(images)\n            B = images.shape[0]\n            x = torch.cat([i.view(B, model.head.no, -1) for i in outputs], dim=2)\n            pred_distri, pred_scores = x.split(split_size=(16*4, NUM_CLASSES), dim=1)\n            pred_scores = pred_scores.permute(0, 2, 1).contiguous()\n            pred_distri = pred_distri.permute(0, 2, 1).contiguous()\n    \n            anchor_points, stride_tensor = make_anchors(outputs, model.head.stride, offset=0.5)\n            anchor_points = anchor_points.to(device)\n            pred_bboxes = criterion.box_decode(anchor_points, pred_distri)\n            pred_bboxes = pred_bboxes.view(-1, 4)\n    \n            pred_scores_flat = pred_scores.view(-1, NUM_CLASSES)\n            pred_classes = pred_scores_flat.argmax(1)\n            pred_conf = pred_scores_flat.max(1).values\n    \n            tp_matrix = build_tp_matrix_torch(pred_bboxes, pred_conf, pred_classes, targets['box'], targets['cls'])\n    \n            all_tp.append(tp_matrix)\n            all_conf.append(pred_conf)\n            all_pred_cls.append(pred_classes)\n            all_target_cls.append(targets['cls'])\n    \n        # Concatenate all batches (GPU)\n        all_tp = torch.cat(all_tp, dim=0)\n        all_conf = torch.cat(all_conf, dim=0)\n        all_pred_cls = torch.cat(all_pred_cls, dim=0)\n        all_target_cls = torch.cat(all_target_cls, dim=0)\n    \n        # Compute mAP (GPU)\n        tp_arr, fp_arr, m_pre, m_rec, map50, mean_ap = compute_ap_torch(\n            all_tp, all_conf, all_pred_cls, all_target_cls\n        )\n    \n        # Hiển thị trên tqdm\n        val_pbar.set_postfix({\n            \"mAP50\": f\"{map50:.4f}\",\n            \"mAP50-95\": f\"{mean_ap:.4f}\"\n        })\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-23T09:14:01.635168Z","iopub.execute_input":"2025-08-23T09:14:01.635538Z","iopub.status.idle":"2025-08-23T09:23:34.504196Z","shell.execute_reply.started":"2025-08-23T09:14:01.635510Z","shell.execute_reply":"2025-08-23T09:23:34.503173Z"}},"outputs":[{"name":"stderr","text":"Epoch 1/10 - Val:   2%|▏         | 2/81 [09:32<6:17:06, 286.41s/it]\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_36/2776168541.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     28\u001b[0m             \u001b[0mpred_conf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpred_scores_flat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m             \u001b[0mtp_matrix\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuild_tp_matrix_torch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred_bboxes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpred_conf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpred_classes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'box'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'cls'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m             \u001b[0mall_tp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtp_matrix\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipykernel_36/1553783507.py\u001b[0m in \u001b[0;36mbuild_tp_matrix_torch\u001b[0;34m(pred_bboxes, pred_conf, pred_classes, gt_bboxes, gt_classes)\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mthr\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miou_thresholds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m         tp_matrix[:, j] = match_detections_torch(\n\u001b[0m\u001b[1;32m     71\u001b[0m             \u001b[0mpred_bboxes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpred_conf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpred_classes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgt_bboxes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgt_classes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miou_threshold\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mthr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m         )\n","\u001b[0;32m/tmp/ipykernel_36/1553783507.py\u001b[0m in \u001b[0;36mmatch_detections_torch\u001b[0;34m(pred_bboxes, pred_conf, pred_classes, gt_bboxes, gt_classes, iou_threshold)\u001b[0m\n\u001b[1;32m     50\u001b[0m         \u001b[0miou_matches\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mious\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0miou_threshold\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m         \u001b[0mmatches\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcls_matches\u001b[0m \u001b[0;34m&\u001b[0m \u001b[0miou_matches\u001b[0m \u001b[0;34m&\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m~\u001b[0m\u001b[0massigned_gt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0mmatches\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0many\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m             \u001b[0mgt_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mious\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mmatches\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m             \u001b[0mtp\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msort_idx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}],"execution_count":94}]}